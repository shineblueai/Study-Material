{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Machine Learning — Assessment\n",
    "\n",
    "This assessment aligns with materials in `Machine-Learning/` (e.g., Linear/Polynomial/Ridge/Lasso/ElasticNet/KNN/SVM/Tree/Ensembles/GB/HistGB/XGBoost notebooks, and datasets such as winequality). Focus: supervised regression workflow, preprocessing, model selection, metrics, bias-variance.\n",
    "\n",
    "Total questions: 25 (10 Theory, 8 Fill-in-the-Blanks, 7 Coding). Difficulty mix: 40% easy, 40% medium, 20% hard.\n"
   ],
   "id": "db87c2d19f51614d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instructions\n",
    "- Answer all questions.\n",
    "- Implement coding tasks using scikit-learn idioms; run asserts.\n",
    "- Keep signatures unchanged.\n",
    "- Solutions at the bottom.\n"
   ],
   "id": "540428c399b0ec69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- All notebooks within `Machine-Learning/`\n",
    "- `winequality-*.csv`\n"
   ],
   "id": "5b50252d7fce2a80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part A — Theory (10)\n",
    "1. What is the difference between parametric and non-parametric models? Give one example each.\n",
    "2. MCQ: Which metric is scale-sensitive and penalizes large errors more? (a) MAE (b) RMSE (c) R^2 (d) Accuracy\n",
    "3. Explain bias-variance tradeoff and relate it to model capacity and regularization.\n",
    "4. Why is it important to perform feature scaling for KNN and SVR?\n",
    "5. MCQ: Which cross-validation strategy preserves target distribution for classification? (a) KFold (b) StratifiedKFold (c) GroupKFold (d) TimeSeriesSplit\n",
    "6. When would you use `PolynomialFeatures`? What are the risks?\n",
    "7. Compare Lasso vs Ridge regularization effects on coefficients.\n",
    "8. What is feature leakage and how can pipelines help prevent it?\n",
    "9. MCQ: Which ensemble reduces variance by averaging many decorrelated models? (a) AdaBoost (b) RandomForest (c) GradientBoosting (d) Logistic Regression\n",
    "10. Explain why `R^2` can be negative on the test set.\n"
   ],
   "id": "8d6eaa75e6f54117"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part B — Fill in the Blanks (8)\n",
    "1. RMSE is the square root of the __________.\n",
    "2. Standardizing features centers them at zero mean and unit __________.\n",
    "3. In `train_test_split`, passing `random_state` ensures __________ splits.\n",
    "4. `Pipeline` ensures that transformations are fit only on the __________ data within CV.\n",
    "5. Lasso tends to drive some coefficients exactly to __________.\n",
    "6. Tree-based models are generally __________ to feature scaling.\n",
    "7. For time-ordered data, prefer __________ cross-validation.\n",
    "8. Hyperparameter search over a discrete grid is implemented by `__________` in scikit-learn.\n"
   ],
   "id": "aadca32b86040035"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part C — Coding Tasks (7)\n",
    "Use scikit-learn and NumPy. We'll synthesize small data for asserts to avoid external files.\n",
    "\n",
    "Tasks:\n",
    "1. `rmse(y_true, y_pred)` — return RMSE.\n",
    "2. `scale_then_knn_reg(X, y, k)` — pipeline: StandardScaler + KNeighborsRegressor; return 5-fold CV mean RMSE (negative MSE route).\n",
    "3. `linreg_r2(X, y)` — fit LinearRegression, return test R^2 using 80/20 split (random_state=0).\n",
    "4. `poly_ridge_score(X, y, degree, alpha)` — pipeline: PolynomialFeatures(degree), StandardScaler, Ridge(alpha); return 3-fold CV mean R^2.\n",
    "5. `feature_importance_rf(X, y, n)` — fit RandomForestRegressor; return indices of top-n features by importance (desc).\n",
    "6. `standardize_columns(X)` — return standardized array (columnwise) with ddof=0; if std=0, output zeros for that column.\n",
    "7. `grid_search_svr(X, y, Cs, gammas)` — pipeline StandardScaler+SVR with grid over `C` and `gamma`; return best params dict.\n"
   ],
   "id": "7afa19c0322efd23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "def scale_then_knn_reg(X, y, k=5):\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('knn', KNeighborsRegressor(n_neighbors=k))\n",
    "    ])\n",
    "    # cross_val_score uses positive score; we convert neg MSE to RMSE\n",
    "    scores = cross_val_score(pipe, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmses = np.sqrt(-scores)\n",
    "    return float(rmses.mean())\n",
    "\n",
    "def linreg_r2(X, y):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    lr = LinearRegression().fit(Xtr, ytr)\n",
    "    yp = lr.predict(Xte)\n",
    "    return float(r2_score(yte, yp))\n",
    "\n",
    "def poly_ridge_score(X, y, degree=2, alpha=1.0):\n",
    "    pipe = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('sc', StandardScaler()),\n",
    "        ('rg', Ridge(alpha=alpha))\n",
    "    ])\n",
    "    scores = cross_val_score(pipe, X, y, cv=3, scoring='r2')\n",
    "    return float(scores.mean())\n",
    "\n",
    "def feature_importance_rf(X, y, n=3, seed=0):\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=seed)\n",
    "    rf.fit(X, y)\n",
    "    imp = rf.feature_importances_\n",
    "    idx = np.argsort(-imp)[:n]\n",
    "    return idx.tolist()\n",
    "\n",
    "def standardize_columns(X):\n",
    "    X = np.asarray(X, float)\n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0)\n",
    "    sd_safe = np.where(sd==0, 1.0, sd)\n",
    "    out = (X - mu) / sd_safe\n",
    "    out[:, sd==0] = 0.0\n",
    "    return out\n",
    "\n",
    "def grid_search_svr(X, y, Cs=(0.1,1,10), gammas=(0.01,0.1,1.0)):\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('svr', SVR())\n",
    "    ])\n",
    "    param_grid = {'svr__C': list(Cs), 'svr__gamma': list(gammas)}\n",
    "    gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "    gs.fit(X, y)\n",
    "    return {'C': float(gs.best_params_['svr__C']), 'gamma': float(gs.best_params_['svr__gamma'])}\n"
   ],
   "id": "fe471e905a6d2e19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Asserts\n",
    "rs = np.random.RandomState(0)\n",
    "X = rs.randn(120, 5)\n",
    "coef = np.array([1.5, -2.0, 0.0, 0.5, 0.0])\n",
    "y = X @ coef + rs.randn(120)*0.3\n",
    "\n",
    "assert round(rmse([0,0,0],[1,1,1]), 5) == round(np.sqrt(1.0), 5)\n",
    "\n",
    "knn_rmse = scale_then_knn_reg(X, y, 3)\n",
    "assert knn_rmse > 0\n",
    "\n",
    "r2 = linreg_r2(X, y)\n",
    "assert -1.0 <= r2 <= 1.0\n",
    "\n",
    "score = poly_ridge_score(X, y, 2, 1.0)\n",
    "assert -1.0 <= score <= 1.0\n",
    "\n",
    "top2 = feature_importance_rf(X, y, 2)\n",
    "assert len(top2) == 2\n",
    "\n",
    "stdX = standardize_columns(X)\n",
    "assert np.allclose(stdX.mean(axis=0), 0, atol=1e-7)\n",
    "\n",
    "best = grid_search_svr(X, y, Cs=(0.1,1.0), gammas=(0.01,0.1))\n",
    "assert set(best.keys()) == {'C','gamma'}\n",
    "\n",
    "print('Machine-Learning asserts passed ✅')\n"
   ],
   "id": "141c239acb3fd654"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Solutions\n",
    "\n",
    "### Theory (sample)\n",
    "1. Parametric (e.g., Linear Regression) assumes finite parameters; non-parametric (e.g., KNN) grows with data.\n",
    "2. (b) RMSE\n",
    "3. Bias decreases with complexity; variance increases — need balance/regularization.\n",
    "4. Distance-based models depend on scale; unscaled features distort distances.\n",
    "5. (b) StratifiedKFold\n",
    "6. When relationships are nonlinear; risk: overfitting and multicollinearity.\n",
    "7. Lasso: sparse coefficients; Ridge: shrinkage without sparsity.\n",
    "8. Leakage occurs when information from test folds leaks into training; pipelines fit transforms only on training.\n",
    "9. (b) RandomForest\n",
    "10. Poor generalization can perform worse than predicting the mean.\n",
    "\n",
    "### Fill blanks\n",
    "1. MSE\n",
    "2. variance\n",
    "3. reproducible\n",
    "4. training\n",
    "5. zero\n",
    "6. insensitive\n",
    "7. TimeSeriesSplit\n",
    "8. GridSearchCV\n"
   ],
   "id": "7088318634bc9a36"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
