{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Activation Functions — Comprehensive Guide (one command per code cell)\n",
    "This notebook-style script lists common activation functions used in deep learning. For each activation, you get:\n",
    "- Formula (plain text), short explanation, domain/range, and derivative note\n",
    "- Minimal examples in pure Python/NumPy, TensorFlow, and PyTorch\n",
    "\n",
    "Conventions in this project:\n",
    "- Markdown cells start with `#%% md`\n",
    "- Code cells start with `#%%`\n",
    "- \"One command per code cell\": every code cell contains exactly one executable command (plus optional comment).\n"
   ],
   "id": "9aa5ad194b02c848"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optional installations (commented)\n",
    "# Uncomment in your environment if needed."
   ],
   "id": "390dc36628be1b75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install numpy tensorflow torch  # install core libs if missing\n",
   "id": "ca65915696080d26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports — one per cell to respect the rule",
   "id": "d0b30db70a6e1c6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import math  # math utilities for scalar formulas\n",
   "id": "44235ebd384fd67a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import numpy as np  # NumPy for vectorized Python examples\n",
   "id": "8211f4f024861017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import tensorflow as tf  # TensorFlow examples (requires tf installed)\n",
   "id": "a141aaa54ee60d3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import torch  # PyTorch examples (requires torch installed)\n",
   "id": "c54bc81747d50ab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample inputs (shared across examples)",
   "id": "a224da8741ddc0e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_np = np.linspace(-5, 5, 11)  # NumPy vector spanning negative to positive\n",
   "id": "4df53eebc7b6eefc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_tf = tf.constant(np.linspace(-5, 5, 11), dtype=tf.float32)  # TensorFlow vector input\n",
   "id": "650319fba41ff240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_torch = torch.linspace(-5, 5, steps=11)  # PyTorch vector input\n",
   "id": "f5bc8578c3643796"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1) Identity / Linear\n",
    "Formula: f(x) = x\n",
    "Explanation: Pass-through; used for outputs in regression.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: f'(x) = 1"
   ],
   "id": "b3e98c2e8fa63442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_np  # Identity in NumPy: returns the same input\n",
   "id": "86b4b36bb8f3d7d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_tf  # Identity in TensorFlow: returns the same tensor\n",
   "id": "9f244f50a2c38bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_torch  # Identity in PyTorch: returns the same tensor\n",
   "id": "b94888dd8acf35d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2) Binary Step (Heaviside)\n",
    "Formula: f(x) = 1 if x >= 0 else 0\n",
    "Explanation: Discontinuous step; not used for backprop (zero gradient almost everywhere).\n",
    "Domain: (-∞, ∞) → Range: {0, 1}\n",
    "Derivative: 0 almost everywhere (undefined at 0)"
   ],
   "id": "847c1c387f11238e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(x_np >= 0).astype(np.float32)  # NumPy binary step using boolean mask\n",
   "id": "a66ec0f6d0307716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.cast(x_tf >= 0.0, tf.float32)  # TensorFlow binary step via cast\n",
   "id": "cfb4c7e332da2f66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(x_torch >= 0).to(dtype=torch.float32)  # PyTorch binary step via boolean to float\n",
   "id": "dfe6059f5fa99ebf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3) Sigmoid (Logistic)\n",
    "Formula: f(x) = 1 / (1 + exp(-x))\n",
    "Explanation: Squashes to (0,1); good for probabilities/binary outputs; can saturate.\n",
    "Domain: (-∞, ∞) → Range: (0, 1)\n",
    "Derivative: f'(x) = f(x)·(1 - f(x))"
   ],
   "id": "594736e6eb7ea32b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "1.0 / (1.0 + np.exp(-x_np))  # NumPy sigmoid\n",
   "id": "3e49e7a1241c09fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.sigmoid(x_tf)  # TensorFlow sigmoid\n",
   "id": "80f77624e1845303"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.sigmoid(x_torch)  # PyTorch sigmoid\n",
   "id": "e51b991d7e7cc9a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4) Tanh\n",
    "Formula: f(x) = (e^x - e^{-x}) / (e^x + e^{-x})\n",
    "Explanation: Zero-centered version of sigmoid; range (-1,1); can still saturate.\n",
    "Domain: (-∞, ∞) → Range: (-1, 1)\n",
    "Derivative: f'(x) = 1 - tanh(x)^2"
   ],
   "id": "a86cf93547f528c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.tanh(x_np)  # NumPy tanh\n",
   "id": "b4d1138ad9e0dfb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.tanh(x_tf)  # TensorFlow tanh\n",
   "id": "7220f3a8c4a2ae13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.tanh(x_torch)  # PyTorch tanh\n",
   "id": "dc3b15a76ac18829"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5) ReLU (Rectified Linear Unit)\n",
    "Formula: f(x) = max(0, x)\n",
    "Explanation: Sparse activations, mitigates vanishing gradients; can \"die\" for negative inputs.\n",
    "Domain: (-∞, ∞) → Range: [0, ∞)\n",
    "Derivative: 1 for x>0, 0 for x<0 (undefined at 0 → pick 0 or 1)"
   ],
   "id": "3674c6b403f4fa4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.maximum(x_np, 0.0)  # NumPy ReLU\n",
   "id": "5062ea92b741b844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.relu(x_tf)  # TensorFlow ReLU\n",
   "id": "386d73079b125b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.relu(x_torch)  # PyTorch ReLU\n",
   "id": "cc5107a6b459e95b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6) Leaky ReLU\n",
    "Formula: f(x) = x if x>0 else α·x, with small α (e.g., 0.01)\n",
    "Explanation: Fixes dying ReLU by allowing small negative slope.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: 1 for x>0, α for x<0"
   ],
   "id": "d9f186de9d43c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.where(x_np > 0, x_np, 0.01 * x_np)  # NumPy Leaky ReLU with alpha=0.01\n",
   "id": "c2cab53e2cc55429"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.leaky_relu(x_tf, alpha=0.01)  # TensorFlow Leaky ReLU\n",
   "id": "7581a05c1fc2328c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.leaky_relu(x_torch, negative_slope=0.01)  # PyTorch Leaky ReLU\n",
   "id": "10f5ab13f586c8ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7) PReLU (Parametric ReLU)\n",
    "Formula: f(x) = x if x>0 else a·x, where a is learned per-channel or per-parameter.\n",
    "Explanation: Learnable negative slope generalizing Leaky ReLU.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: 1 for x>0, a for x<0 (a is learnable)"
   ],
   "id": "2b6ae40b80d987b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.keras.layers.PReLU()(x_tf)  # TensorFlow PReLU layer applied to x (creates learnable alpha)\n",
   "id": "e5451dcc542956aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.PReLU()(x_torch)  # PyTorch PReLU module applied to x (learnable parameter)\n",
   "id": "84fb3fbce0fc2af2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 8) ELU (Exponential Linear Unit)\n",
    "Formula: f(x) = x if x>0 else α·(exp(x) - 1)\n",
    "Explanation: Negative values push mean activations toward zero; smooth negative part.\n",
    "Domain: (-∞, ∞) → Range: (-α, ∞)\n",
    "Derivative: 1 for x>0; f(x)+α for x<=0 (scaled by α)"
   ],
   "id": "b9a2975188d7f2ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.where(x_np > 0, x_np, 1.0 * (np.exp(x_np) - 1.0))  # NumPy ELU with alpha=1.0\n",
   "id": "576b131cd3f0f4d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.elu(x_tf)  # TensorFlow ELU (alpha=1)\n",
   "id": "f8c33521ce0625f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.elu(x_torch, alpha=1.0)  # PyTorch ELU\n",
   "id": "5e42ed3bdaf25636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 9) SELU (Scaled ELU)\n",
    "Formula: f(x) = λ·(x) for x>0; f(x) = λ·(α·(exp(x) - 1)) for x<=0\n",
    "Explanation: Self-normalizing activations; use with LeCun normal init and AlphaDropout.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞) (scaled)\n",
    "Derivative: piecewise as ELU, scaled by λ"
   ],
   "id": "b2d957611b93a798"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.selu(x_tf)  # TensorFlow SELU (uses fixed λ and α constants)\n",
   "id": "7fc1fb43758b4a1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.selu(x_torch)  # PyTorch SELU\n",
   "id": "9f9da4eee5651a8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 10) GELU (Gaussian Error Linear Unit)\n",
    "Formula (exact): f(x) = x·Φ(x), where Φ is Gaussian CDF. Approx: 0.5·x·(1 + tanh(√(2/π)·(x + 0.044715·x^3)))\n",
    "Explanation: Smooth, stochastic regularization interpretation; performs well in Transformers.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: involves Gaussian pdf/cdf; smooth"
   ],
   "id": "acf03b77ece51dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "0.5 * x_np * (1.0 + np.tanh(np.sqrt(2.0 / np.pi) * (x_np + 0.044715 * (x_np ** 3))))  # NumPy GELU (approx)\n",
   "id": "31eb4007030d7f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.gelu(x_tf, approximate=True)  # TensorFlow GELU (approximate)\n",
   "id": "6edaa87d29fb3c52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.gelu(x_torch)  # PyTorch GELU\n",
   "id": "ee039b8eb0e1d394"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 11) Softplus\n",
    "Formula: f(x) = ln(1 + exp(x))\n",
    "Explanation: Smooth approximation to ReLU; always positive.\n",
    "Domain: (-∞, ∞) → Range: (0, ∞)\n",
    "Derivative: f'(x) = sigmoid(x)"
   ],
   "id": "d362ca28ac8aa90e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.log1p(np.exp(x_np))  # NumPy Softplus (uses log1p for stability)\n",
   "id": "9293c41b0e1cfd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.softplus(x_tf)  # TensorFlow Softplus\n",
   "id": "75aabcf81338f7c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.softplus(x_torch)  # PyTorch Softplus\n",
   "id": "a9b3b7f3823e4e2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 12) Softsign\n",
    "Formula: f(x) = x / (1 + |x|)\n",
    "Explanation: Smooth squashing similar to tanh but with polynomial tails.\n",
    "Domain: (-∞, ∞) → Range: (-1, 1)\n",
    "Derivative: f'(x) = 1 / (1 + |x|)^2"
   ],
   "id": "275707dabf197133"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_np / (1.0 + np.abs(x_np))  # NumPy Softsign\n",
   "id": "3ce8a03929f2ca02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.softsign(x_tf)  # TensorFlow Softsign\n",
   "id": "64ebf74c140b048e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.softsign(x_torch)  # PyTorch Softsign\n",
   "id": "dd9ece4b694ffbf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 13) Swish / SiLU\n",
    "Formula: f(x) = x · sigmoid(x)\n",
    "Explanation: Self-gated smooth nonlinearity; aka SiLU in TF/PyTorch.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: f'(x) = sigmoid(x) + x·sigmoid(x)·(1 - sigmoid(x))"
   ],
   "id": "edd512dbfee7861d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_np / (1.0 + np.exp(-x_np))  # NumPy Swish (x * sigmoid(x))\n",
   "id": "68279dd8b1073767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.silu(x_tf)  # TensorFlow SiLU (Swish)\n",
   "id": "86e5843e146cbb24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.silu(x_torch)  # PyTorch SiLU (Swish)\n",
   "id": "d4e1559a50f5f9d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 14) Mish\n",
    "Formula: f(x) = x · tanh(softplus(x))\n",
    "Explanation: Smooth nonmonotonic; sometimes outperforms Swish.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: complex; uses tanh and sigmoid components"
   ],
   "id": "9a4b6f6e9fc04069"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_np * np.tanh(np.log1p(np.exp(x_np)))  # NumPy Mish (softplus then tanh)\n",
   "id": "ea0661f7209e760a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_tf * tf.math.tanh(tf.nn.softplus(x_tf))  # TensorFlow Mish (composed ops)\n",
   "id": "3dc910cd35476cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.mish(x_torch)  # PyTorch Mish\n",
   "id": "8db7d0b7e8ff1884"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 15) Hard Sigmoid\n",
    "Formula: f(x) = clip((x + 3) / 6, 0, 1)\n",
    "Explanation: Piecewise-linear approx of sigmoid; cheap.\n",
    "Domain: (-∞, ∞) → Range: [0, 1]\n",
    "Derivative: 1/6 in linear region; 0 in saturated regions"
   ],
   "id": "65aeaffaea841d48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.clip((x_np + 3.0) / 6.0, 0.0, 1.0)  # NumPy Hard Sigmoid\n",
   "id": "d2fe4f54917e3499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.keras.activations.hard_sigmoid(x_tf)  # TensorFlow hard sigmoid\n",
   "id": "b70e12af60e3612b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.hardsigmoid(x_torch)  # PyTorch hard sigmoid\n",
   "id": "e4d05d22588c08b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 16) Hard Swish\n",
    "Formula: f(x) = x · hard_sigmoid(x) = x · clip((x + 3)/6, 0, 1)\n",
    "Explanation: Efficient Swish approximation used in MobileNetV3.\n",
    "Domain: (-∞, ∞) → Range: (-∞, ∞)\n",
    "Derivative: piecewise; linear region scaled by x"
   ],
   "id": "d7530d9dfad40d2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "x_np * np.clip((x_np + 3.0) / 6.0, 0.0, 1.0)  # NumPy Hard Swish\n",
   "id": "7b3099e718aebb85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.hard_swish(x_tf)  # TensorFlow hard swish\n",
   "id": "bf6ad811d94f4569"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.hardswish(x_torch)  # PyTorch hard swish\n",
   "id": "1af8c5a9f1c38411"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 17) ReLU6\n",
    "Formula: f(x) = min(max(0, x), 6)\n",
    "Explanation: Clipped ReLU used in mobile networks to bound activations.\n",
    "Domain: (-∞, ∞) → Range: [0, 6]\n",
    "Derivative: 1 for 0<x<6; 0 otherwise (undefined at 0 and 6)"
   ],
   "id": "eaafdf717a4c5e44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.minimum(np.maximum(x_np, 0.0), 6.0)  # NumPy ReLU6\n",
   "id": "dce67386bb626bbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.relu6(x_tf)  # TensorFlow ReLU6\n",
   "id": "cd5ca2f116ed2aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.relu6(x_torch)  # PyTorch ReLU6\n",
   "id": "8017b4bb5bd263d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 18) HardTanh\n",
    "Formula: f(x) = -1 if x<-1; f(x) = x if -1<=x<=1; f(x) = 1 if x>1\n",
    "Explanation: Clipped tanh approximation, piecewise linear.\n",
    "Domain: (-∞, ∞) → Range: [-1, 1]\n",
    "Derivative: 1 in linear region; 0 in saturated regions"
   ],
   "id": "cd1a4ae65afba1ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.clip(x_np, -1.0, 1.0)  # NumPy HardTanh (clamp)\n",
   "id": "7340561b096550b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.clip_by_value(x_tf, -1.0, 1.0)  # TensorFlow clamp to [-1, 1]\n",
   "id": "75c17f9e6f026b33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.hardtanh(x_torch, min_val=-1.0, max_val=1.0)  # PyTorch HardTanh\n",
   "id": "9f8aa691eca7d344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 19) LogSigmoid\n",
    "Formula: f(x) = log(1 / (1 + exp(-x))) = -softplus(-x)\n",
    "Explanation: Numerically stable log of sigmoid; useful in NCE and binary log-likelihoods.\n",
    "Domain: (-∞, ∞) → Range: (-∞, 0)\n",
    "Derivative: 1 - sigmoid(-x) = sigmoid(x) - 1 for some forms"
   ],
   "id": "ee3e7adbdeedf16c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "-(np.log1p(np.exp(-x_np)))  # NumPy LogSigmoid (stable via log1p)\n",
   "id": "1ae1a210a77bbde8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.math.log_sigmoid(x_tf)  # TensorFlow LogSigmoid\n",
   "id": "b9d7f501d670b265"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.logsigmoid(x_torch)  # PyTorch LogSigmoid\n",
   "id": "f4ae0e40e50984bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 20) Softmax (multi-class output)\n",
    "Formula: f_i(x) = exp(x_i) / Σ_j exp(x_j)\n",
    "Explanation: Converts logits to probability distribution over classes.\n",
    "Domain: ℝ^K → Range: simplex (nonnegative, sums to 1)\n",
    "Derivative: Jacobian involves p_i(δ_{ij} - p_j)"
   ],
   "id": "5265a556721ce169"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.exp(x_np - np.max(x_np)) / np.sum(np.exp(x_np - np.max(x_np)))  # NumPy softmax over 1D vector (stable shift)\n",
   "id": "8c7e8d97ac1fde7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tf.nn.softmax(x_tf)  # TensorFlow Softmax over last dim (1D vector here)\n",
   "id": "298e0d14d401852e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.nn.functional.softmax(x_torch, dim=-1)  # PyTorch Softmax over last dim\n",
   "id": "e5d5374bde0aff5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes and Tips\n",
    "1. Numerical stability: prefer `log1p`, max-shift in softmax, and library functions (`tf.nn.*`, `torch.nn.functional.*`).\n",
    "2. Initialization & normalization: SELU requires LeCun normal init and AlphaDropout; ReLU-family often pairs with He init.\n",
    "3. Output activations: Use sigmoid for binary outputs, softmax for multi-class logits, identity for regression.\n",
    "4. Derivatives listed are for reference; autodiff frameworks compute gradients automatically in TF/PyTorch.\n"
   ],
   "id": "13c05efb71eada93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
