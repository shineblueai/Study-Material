{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding and BPE Tokenization — Interactive Demo (Gradio)\n",
    "\n",
    "This notebook walks step-by-step through:\n",
    "\n",
    "1. Installing required libraries (Gradio for UI, Tokenizers for BPE, NumPy for math).\n",
    "2. Implementing a minimal Byte-Pair Encoding (BPE) tokenizer using `huggingface/tokenizers`.\n",
    "3. Building a simple embedding matrix and producing token embeddings for an input sentence.\n",
    "4. Displaying the number of tokens created by BPE and the token→id mapping (tokenization).\n",
    "5. Saving a new vocabulary file built from the tokens encountered.\n",
    "6. Computing sinusoidal positional encodings, as described in the original Transformer paper:\n",
    "   - Even dimensions: `PE[pos, 2i]   = sin(pos / (10000^(2i/d_model)))`\n",
    "   - Odd  dimensions: `PE[pos, 2i+1] = cos(pos / (10000^(2i/d_model)))`\n",
    "\n",
    "You will be able to type a sentence and see:\n",
    "- The embeddings for its tokens,\n",
    "- The token count,\n",
    "- The token→id mapping,\n",
    "- The saved vocabulary file generated from these tokens,\n",
    "- The positional encodings for the sequence.\n",
    "\n",
    "All code cells are thoroughly documented line-by-line.\n"
   ],
   "id": "616583fe1ee9cb73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:20.218588Z",
     "start_time": "2025-11-19T15:13:19.503326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required libraries using pip. Each line is documented.\n",
    "# The exclamation mark tells Jupyter to run a shell command.\n",
    "# We install:\n",
    "#  - gradio: to build an interactive UI right inside this notebook.\n",
    "#  - tokenizers: Hugging Face's fast BPE implementation for tokenization.\n",
    "#  - numpy: numerical computations for embeddings and positional encodings.\n",
    "!pip -q install gradio tokenizers numpy\n"
   ],
   "id": "b2d75e0196cd13e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.340750Z",
     "start_time": "2025-11-19T15:13:20.223922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import standard libraries and third-party packages. Each import is commented.\n",
    "import os  # Operating system interfaces (used for file paths and saving vocab).\n",
    "from typing import Dict, List, Tuple  # Type hints to clarify function inputs/outputs.\n",
    "\n",
    "# We attempt to import third-party packages and, if missing, install them on the fly.\n",
    "try:\n",
    "    import numpy as np  # Core numerical library for arrays, random numbers, and math ops.\n",
    "except ImportError:\n",
    "    import sys, subprocess  # Fallback to install numpy if not present.\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy\"])\n",
    "    import numpy as np\n",
    "\n",
    "# Try to import gradio, install if it's missing so this cell never fails when run first.\n",
    "try:\n",
    "    import gradio as gr  # Gradio provides a simple UI for interacting with Python functions.\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio\"])\n",
    "    import gradio as gr\n",
    "\n",
    "# Try to import tokenizers, install if it's missing.\n",
    "try:\n",
    "    from tokenizers import Tokenizer  # The main Tokenizer object.\n",
    "    from tokenizers.models import BPE  # The BPE model specification.\n",
    "    from tokenizers.trainers import BpeTrainer  # Trainer to learn merges/vocab.\n",
    "    from tokenizers.pre_tokenizers import ByteLevel  # Byte-level pre-tokenization (robust for text).\n",
    "    from tokenizers.decoders import ByteLevel as ByteLevelDecoder  # Decoder to map ids back to text.\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tokenizers\"])\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    from tokenizers.pre_tokenizers import ByteLevel\n",
    "    from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "\n",
    "# Try to import reportlab for PDF generation; install on-the-fly if missing.\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import letter  # Standard US letter page size.\n",
    "    from reportlab.pdfgen import canvas  # Canvas API to draw text on PDF pages.\n",
    "    from reportlab.lib.units import inch  # Convenience for positioning.\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"reportlab\"])\n",
    "    from reportlab.lib.pagesizes import letter\n",
    "    from reportlab.pdfgen import canvas\n",
    "    from reportlab.lib.units import inch\n",
    "\n",
    "# Set a consistent random seed so results are reproducible between runs.\n",
    "np.random.seed(42)\n"
   ],
   "id": "2ef97d5fb5973013",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.352037Z",
     "start_time": "2025-11-19T15:13:21.349557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper: Train a small BPE tokenizer from an iterator of texts.\n",
    "# We use ByteLevel pre-tokenization which operates on bytes (robust and GPT-like).\n",
    "def train_bpe_tokenizer(texts: List[str],\n",
    "                        vocab_size: int = 2000,\n",
    "                        special_tokens: List[str] = (\n",
    "                            \"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"\n",
    "                        )) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Trains a BPE tokenizer on the provided texts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : List[str]\n",
    "        Sentences or documents to learn BPE vocab/merges from.\n",
    "    vocab_size : int\n",
    "        Maximum vocabulary size (including special tokens).\n",
    "    special_tokens : List[str]\n",
    "        Special tokens to reserve at the start of the vocabulary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tokenizer\n",
    "        A trained Hugging Face `Tokenizer` configured for BPE.\n",
    "    \"\"\"\n",
    "    # Initialize a BPE model with an unknown token (used when encountering OOV tokens).\n",
    "    model = BPE(unk_token=\"[UNK]\")\n",
    "\n",
    "    # Create the Tokenizer object with this model.\n",
    "    tokenizer = Tokenizer(model)\n",
    "\n",
    "    # Use byte-level pre-tokenizer to split text into a consistent stream of bytes.\n",
    "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n",
    "\n",
    "    # Use a matching byte-level decoder to reconstruct text if needed.\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    # Trainer defines training parameters like vocab size and special tokens.\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=list(special_tokens))\n",
    "\n",
    "    # Train the tokenizer on the provided texts iterator.\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "    # Return the trained tokenizer ready for encoding.\n",
    "    return tokenizer\n"
   ],
   "id": "476a0394b6110444",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.364460Z",
     "start_time": "2025-11-19T15:13:21.362208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper: Create an embedding matrix and look up embeddings for a token id sequence.\n",
    "def build_embeddings_and_lookup(vocab_size: int,\n",
    "                                 d_model: int,\n",
    "                                 token_ids: List[int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Builds a random embedding matrix and returns the matrix and the embeddings\n",
    "    corresponding to the provided token ids.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        Size of the vocabulary (number of rows in the embedding matrix).\n",
    "    d_model : int\n",
    "        Dimensionality of each embedding vector (number of columns).\n",
    "    token_ids : List[int]\n",
    "        Sequence of token indices to look up embeddings for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        (embedding_matrix, token_embeddings) where:\n",
    "        - embedding_matrix has shape (vocab_size, d_model)\n",
    "        - token_embeddings has shape (len(token_ids), d_model)\n",
    "    \"\"\"\n",
    "    # Create a random embedding matrix with small values (mean=0, std ~ 0.02 typical for NLP inits).\n",
    "    embedding_matrix = np.random.normal(loc=0.0, scale=0.02, size=(vocab_size, d_model)).astype(np.float32)\n",
    "\n",
    "    # Convert token_ids to a NumPy array for indexing the embedding matrix.\n",
    "    token_ids_array = np.array(token_ids, dtype=np.int64)\n",
    "\n",
    "    # Gather the rows corresponding to each token id to get their embeddings.\n",
    "    token_embeddings = embedding_matrix[token_ids_array]\n",
    "\n",
    "    # Return both the full matrix (for inspection) and the looked-up embeddings.\n",
    "    return embedding_matrix, token_embeddings\n"
   ],
   "id": "2edadc3ea56e2fcb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.378445Z",
     "start_time": "2025-11-19T15:13:21.375900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper: Compute sinusoidal positional encodings as used in Transformers.\n",
    "def positional_encoding(seq_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes a (seq_len, d_model) positional encoding matrix using sinusoidal functions:\n",
    "\n",
    "      PE[pos, 2i]   = sin(pos / (10000^(2i/d_model)))\n",
    "      PE[pos, 2i+1] = cos(pos / (10000^(2i/d_model)))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq_len : int\n",
    "        Length of the sequence (number of positions).\n",
    "    d_model : int\n",
    "        Embedding dimension of the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Positional encodings of shape (seq_len, d_model).\n",
    "    \"\"\"\n",
    "    # Create an array of positions [0, 1, 2, ..., seq_len-1] with shape (seq_len, 1).\n",
    "    positions = np.arange(seq_len, dtype=np.float32)[:, np.newaxis]\n",
    "\n",
    "    # Create an array of even indices [0, 2, 4, ..., d_model-2] with shape (1, d_model/2).\n",
    "    i = np.arange(0, d_model, 2, dtype=np.float32)[np.newaxis, :]\n",
    "\n",
    "    # Compute the denominator term 10000^(2i/d_model) as in the paper.\n",
    "    denom = np.power(10000.0, i / d_model)\n",
    "\n",
    "    # Compute the angle rates = positions / denom with broadcasting to shape (seq_len, d_model/2).\n",
    "    angle_rates = positions / denom\n",
    "\n",
    "    # Initialize PE matrix with zeros for both even and odd columns.\n",
    "    pe = np.zeros((seq_len, d_model), dtype=np.float32)\n",
    "\n",
    "    # Even indices (0, 2, 4, ...): apply sine.\n",
    "    pe[:, 0::2] = np.sin(angle_rates)\n",
    "\n",
    "    # Odd indices (1, 3, 5, ...): apply cosine.\n",
    "    pe[:, 1::2] = np.cos(angle_rates)\n",
    "\n",
    "    # Return the completed positional encoding matrix.\n",
    "    return pe\n"
   ],
   "id": "7028e9993a492285",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.393363Z",
     "start_time": "2025-11-19T15:13:21.388822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper: Create a concise PDF report capturing inputs, calculations, and results.\n",
    "def create_pdf_report(\n",
    "    sentence: str,\n",
    "    d_model: int,\n",
    "    tokens: List[str],\n",
    "    token_ids: List[int],\n",
    "    token_embeddings: np.ndarray,\n",
    "    positional_enc: np.ndarray,\n",
    "    summed_matrix: np.ndarray,\n",
    "    out_path: str,\n",
    "    preview_rows: int = 6,\n",
    "    preview_cols: int = 8,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Builds a PDF report summarizing the pipeline:\n",
    "      - Input sentence and d_model\n",
    "      - Tokens and IDs\n",
    "      - Shapes of matrices\n",
    "      - A small preview (top-left corner) of embeddings, positional encodings, and the summed matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The input sentence provided by the user.\n",
    "    d_model : int\n",
    "        The embedding/hidden dimension used.\n",
    "    tokens : List[str]\n",
    "        Token strings from BPE.\n",
    "    token_ids : List[int]\n",
    "        Corresponding token ids.\n",
    "    token_embeddings : np.ndarray\n",
    "        Raw token embeddings (seq_len, d_model).\n",
    "    positional_enc : np.ndarray\n",
    "        Positional encodings (seq_len, d_model).\n",
    "    summed_matrix : np.ndarray\n",
    "        Result of sqrt(d_model) * token_embeddings + positional_enc.\n",
    "    out_path : str\n",
    "        Full file path where the PDF should be saved.\n",
    "    preview_rows : int\n",
    "        Number of rows to preview for matrices.\n",
    "    preview_cols : int\n",
    "        Number of columns to preview for matrices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The path to the created PDF file.\n",
    "    \"\"\"\n",
    "    c = canvas.Canvas(out_path, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    def writeln(text, x=1*inch, y=None, leading=14):\n",
    "        nonlocal current_y\n",
    "        if y is not None:\n",
    "            current_y = y\n",
    "        # Wrap long lines manually by splitting if needed\n",
    "        max_chars = 95\n",
    "        lines = [text[i:i+max_chars] for i in range(0, len(text), max_chars)] if len(text) > max_chars else [text]\n",
    "        for line in lines:\n",
    "            c.drawString(x, current_y, line)\n",
    "            current_y -= leading\n",
    "\n",
    "    def write_matrix_preview(title, mat: np.ndarray):\n",
    "        nonlocal current_y\n",
    "        writeln(title)\n",
    "        r = min(preview_rows, mat.shape[0])\n",
    "        k = min(preview_cols, mat.shape[1])\n",
    "        # Header\n",
    "        header = \"cols 0..{} (rounded to 4 dp)\".format(k-1)\n",
    "        writeln(header)\n",
    "        for i in range(r):\n",
    "            row_vals = \", \".join(f\"{v:.4f}\" for v in mat[i, :k])\n",
    "            writeln(f\"row {i}: [ {row_vals} ]\")\n",
    "            if current_y < 1*inch:\n",
    "                c.showPage()\n",
    "                current_y = height - 1*inch\n",
    "\n",
    "    current_y = height - 1*inch\n",
    "    writeln(\"Positional Encoding & BPE — Report\", y=current_y)\n",
    "    writeln(\"Reference: Vaswani et al., 2017 — 'Attention Is All You Need'\")\n",
    "    writeln(\"\")\n",
    "    writeln(f\"Input sentence: {sentence}\")\n",
    "    writeln(f\"d_model: {d_model}\")\n",
    "    writeln(f\"Token count: {len(token_ids)}\")\n",
    "    writeln(f\"Tokens: {tokens}\")\n",
    "    writeln(f\"Token IDs: {token_ids}\")\n",
    "    writeln(\"\")\n",
    "\n",
    "    # Shapes and formula\n",
    "    writeln(\"Shapes:\")\n",
    "    writeln(f\" - token_embeddings: {token_embeddings.shape}\")\n",
    "    writeln(f\" - positional_enc:   {positional_enc.shape}\")\n",
    "    writeln(f\" - summed_matrix:     {summed_matrix.shape}\")\n",
    "    writeln(\"\")\n",
    "    writeln(\"We compute: X = sqrt(d_model) * E + PE, where E are token embeddings and PE are sinusoidal encodings.\")\n",
    "    writeln(\"\")\n",
    "\n",
    "    # Previews\n",
    "    write_matrix_preview(\"Preview: token embeddings (E)\", token_embeddings)\n",
    "    write_matrix_preview(\"Preview: positional encodings (PE)\", positional_enc)\n",
    "    write_matrix_preview(\"Preview: X = sqrt(d_model) * E + PE\", summed_matrix)\n",
    "\n",
    "    c.showPage()\n",
    "    c.save()\n",
    "    return out_path\n"
   ],
   "id": "7330ea1369fab876",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.408079Z",
     "start_time": "2025-11-19T15:13:21.403377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core processing function used by the Gradio UI.\n",
    "def process_sentence(sentence: str, d_model: int = 64):\n",
    "    \"\"\"\n",
    "    Given an input sentence and embedding size (d_model), this function:\n",
    "      1) Trains a small BPE tokenizer on the sentence itself (for demo purposes).\n",
    "      2) Encodes the sentence to token ids and collects token strings.\n",
    "      3) Builds a random embedding matrix and looks up embeddings for these token ids.\n",
    "      4) Saves a new vocabulary file built from the tokenizer's learned vocab.\n",
    "      5) Computes sinusoidal positional encodings for the sequence length.\n",
    "\n",
    "    Returns a tuple of objects compatible with Gradio components in this order:\n",
    "      - embeddings_df: 2D list (seq_len x d_model) initial token embedding values.\n",
    "      - posenc_df: 2D list (seq_len x d_model) positional encoding values.\n",
    "      - summed_df: 2D list (seq_len x d_model) for X = sqrt(d_model) * E + PE.\n",
    "      - token_count: integer number of tokens.\n",
    "      - token_map_df: 2D list mapping token string -> token id.\n",
    "      - vocab_file_path: path to the saved vocab JSON file.\n",
    "      - pdf_report_path: path to a generated PDF capturing inputs and results.\n",
    "    \"\"\"\n",
    "    # Fallback to a default sentence if user input is empty to avoid training on nothing.\n",
    "    if not sentence or not sentence.strip():\n",
    "        sentence = \"Transformers are powerful sequence models!\"\n",
    "\n",
    "    # 1) Train a small BPE tokenizer on the provided sentence (demo-only training).\n",
    "    tokenizer = train_bpe_tokenizer([sentence], vocab_size=256)\n",
    "\n",
    "    # 2) Encode the sentence to get token ids and attention offsets.\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    token_ids = encoding.ids  # List of integers representing tokens.\n",
    "    tokens = encoding.tokens  # Corresponding token strings.\n",
    "\n",
    "    # Number of tokens created by BPE for the input sentence.\n",
    "    token_count = len(token_ids)\n",
    "\n",
    "    # Create a token→id mapping table (list of [token, id]) for human-readable display.\n",
    "    token_map_df = [[tok, int(tid)] for tok, tid in zip(tokens, token_ids)]\n",
    "\n",
    "    # 3) Build a random embedding matrix and look up embeddings for the token ids.\n",
    "    vocab_size = len(tokenizer.get_vocab())  # Size of the learned vocabulary.\n",
    "    _, token_embeddings = build_embeddings_and_lookup(vocab_size=vocab_size,\n",
    "                                                     d_model=int(d_model),\n",
    "                                                     token_ids=token_ids)\n",
    "\n",
    "    # Convert embeddings to a nested list for Gradio Dataframe display (initial embeddings E).\n",
    "    embeddings_df = token_embeddings.astype(float).tolist()\n",
    "\n",
    "    # 4) Save the vocabulary learned by the tokenizer to a JSON file.\n",
    "    #    We store it under the current directory with a clear name.\n",
    "    vocab_dict: Dict[str, int] = tokenizer.get_vocab()\n",
    "    # Sort by id for readability (tokenizers may return unsorted dict).\n",
    "    vocab_items = sorted(vocab_dict.items(), key=lambda kv: kv[1])\n",
    "\n",
    "    # Ensure the output directory exists (use the same folder as this notebook).\n",
    "    out_dir = os.path.abspath(\".\")\n",
    "    vocab_file_path = os.path.join(out_dir, \"demo_vocab.json\")\n",
    "\n",
    "    # Write vocab to JSON using the standard library.\n",
    "    import json  # Standard library for JSON serialization.\n",
    "    with open(vocab_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({k: int(v) for k, v in vocab_items}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 5) Compute positional encodings for the sequence length token_count.\n",
    "    pe = positional_encoding(seq_len=token_count, d_model=int(d_model))\n",
    "\n",
    "    # Compute X = sqrt(d_model) * E + PE, as in the Transformer paper (Vaswani et al., 2017).\n",
    "    emb_scaled = token_embeddings * np.sqrt(float(d_model))\n",
    "    summed = emb_scaled + pe\n",
    "\n",
    "    # Prepare tables for display.\n",
    "    posenc_df = pe.astype(float).tolist()\n",
    "    summed_df = summed.astype(float).tolist()\n",
    "\n",
    "    # 6) Create a PDF report with a concise summary and previews.\n",
    "    out_dir = os.path.abspath(\".\")\n",
    "    pdf_file_path = os.path.join(out_dir, \"positional_encoding_report.pdf\")\n",
    "    try:\n",
    "        _ = create_pdf_report(\n",
    "            sentence=sentence,\n",
    "            d_model=int(d_model),\n",
    "            tokens=tokens,\n",
    "            token_ids=token_ids,\n",
    "            token_embeddings=token_embeddings,\n",
    "            positional_enc=pe,\n",
    "            summed_matrix=summed,\n",
    "            out_path=pdf_file_path,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # If PDF generation fails for any reason, we still want the app to work.\n",
    "        # In that case, we create a small text file explaining the error.\n",
    "        pdf_file_path = os.path.join(out_dir, \"positional_encoding_report_failed.txt\")\n",
    "        with open(pdf_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"PDF generation failed: {e}\\n\")\n",
    "            f.write(\"Proceeding without the PDF preview.\\n\")\n",
    "\n",
    "    # Return all outputs in the order expected by the Gradio interface.\n",
    "    return embeddings_df, posenc_df, summed_df, int(token_count), token_map_df, vocab_file_path, pdf_file_path\n"
   ],
   "id": "d16b0867b950240b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:13:21.548214Z",
     "start_time": "2025-11-19T15:13:21.418232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build the Gradio Interface. Each component and parameter is explained.\n",
    "# Define input components:\n",
    "#  - A Textbox for entering the sentence.\n",
    "#  - A Slider to choose d_model (embedding dimension).\n",
    "inp_sentence = gr.Textbox(label=\"Enter a sentence\",\n",
    "                          placeholder=\"Type any sentence to tokenize with BPE...\")\n",
    "inp_dmodel = gr.Slider(minimum=16, maximum=512, step=16, value=64,\n",
    "                       label=\"Embedding dimension (d_model)\")\n",
    "\n",
    "# Define output components in the requested order:\n",
    "out_embeddings = gr.Dataframe(label=\"Initial Token Embeddings E (rows=tokens, cols=d_model)\")\n",
    "out_posenc = gr.Dataframe(label=\"Positional Encodings PE (rows=positions, cols=d_model)\")\n",
    "out_summed = gr.Dataframe(label=\"X = sqrt(d_model) * E + PE (rows=tokens, cols=d_model)\")\n",
    "out_token_count = gr.Number(label=\"Number of BPE tokens\")\n",
    "out_token_map = gr.Dataframe(headers=[\"token\", \"id\"],\n",
    "                             label=\"Token→ID mapping (BPE tokenization)\")\n",
    "out_vocab_file = gr.File(label=\"Saved vocabulary file (JSON)\")\n",
    "out_pdf = gr.File(label=\"Download Report (PDF)\")\n",
    "\n",
    "# Create the Interface, binding inputs to the 'process_sentence' function and mapping outputs.\n",
    "demo = gr.Interface(\n",
    "    fn=process_sentence,\n",
    "    inputs=[inp_sentence, inp_dmodel],\n",
    "    outputs=[\n",
    "        out_embeddings,   # initial embeddings E\n",
    "        out_posenc,       # positional encodings PE\n",
    "        out_summed,       # X = sqrt(d_model) * E + PE\n",
    "        out_token_count,  # token count\n",
    "        out_token_map,    # token->id map\n",
    "        out_vocab_file,   # vocab JSON\n",
    "        out_pdf           # PDF report\n",
    "    ],\n",
    "    title=\"BPE Tokenization, Embeddings, Positional Encodings, and Summation\",\n",
    "    description=(\n",
    "        \"Enter a sentence to see BPE tokenization, initial token embeddings (E), sinusoidal positional encodings (PE),\\n\"\n",
    "        \"their sum X = sqrt(d_model) * E + PE as in 'Attention Is All You Need', a saved vocab, and a downloadable PDF report.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Launch the app in the notebook (inline).\n",
    "# share=False keeps it local; set share=True if you want a public link (not needed here).\n",
    "demo.launch(share=False)\n"
   ],
   "id": "199801f8043b950",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
