{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Learning — Assessment\n",
    "\n",
    "This assessment aligns with materials in `Deep-Learning/Code - Notes` and `Deep-Learning/Master - Notes` (Transformers, RNNs, CNNs, Attention, TensorFlow/PyTorch basics). Focus: foundational theory, architectures, training dynamics, and small coding utilities (framework-light).\n",
    "\n",
    "Total questions: 25 (10 Theory, 8 Fill-in-the-Blanks, 7 Coding). Difficulty mix: 40% easy, 40% medium, 20% hard.\n"
   ],
   "id": "35ba57c99b829b88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instructions\n",
    "- Answer all questions.\n",
    "- Coding tasks are framework-agnostic or NumPy-based to avoid heavy dependencies; asserts included.\n",
    "- Solutions provided at bottom.\n"
   ],
   "id": "3481f80c74e856f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- Code/notes across `Deep-Learning/Code - Notes` and `Deep-Learning/Master - Notes` (e.g., Transformer, RNN/LSTM, CNN, Positional Encoding, TensorFlow/PyTorch overviews)\n"
   ],
   "id": "ee7fc9c0e6cf3e8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part A — Theory (10)\n",
    "1. Explain the difference between training loss and validation loss. What does divergence indicate?\n",
    "2. MCQ: Which activation helps mitigate vanishing gradients? (a) sigmoid (b) tanh (c) ReLU (d) linear\n",
    "3. Describe how backpropagation uses the chain rule to update weights.\n",
    "4. What is overfitting? Name three techniques to reduce it in deep nets.\n",
    "5. MCQ: In attention, the output is weighted sum of (a) queries (b) keys (c) values (d) biases\n",
    "6. Contrast RNN, LSTM, and GRU with respect to long-term dependency handling.\n",
    "7. What is positional encoding in Transformers and why is it needed?\n",
    "8. Explain the concept of teacher forcing in seq2seq training and a potential downside.\n",
    "9. MCQ: BatchNorm typically (a) speeds convergence (b) eliminates need for LR tuning (c) prevents overfitting always (d) replaces dropout\n",
    "10. Why do CNNs share weights spatially? What benefit does this confer?\n"
   ],
   "id": "a9d2f70534254bf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part B — Fill in the Blanks (8)\n",
    "1. The gradient descent update is `w ← w − η * ______`.\n",
    "2. Dropout randomly sets activations to zero during ______.\n",
    "3. In self-attention, the similarity between query and key is computed before applying ______ over scores.\n",
    "4. In LSTM, the gate that controls memory content removal is the ______ gate.\n",
    "5. The Transformer replaces recurrence with ______ mechanisms.\n",
    "6. To stabilize training, gradients may be clipped by ______.\n",
    "7. In convolution, the operation uses a learnable ______ (a small matrix) sliding over the input.\n",
    "8. Layer normalization normalizes across the ______ dimension for each sample.\n"
   ],
   "id": "12362af3618a1600"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part C — Coding Tasks (7)\n",
    "Implement with NumPy. Run asserts.\n",
    "\n",
    "Tasks:\n",
    "1. `relu(x)` — elementwise ReLU.\n",
    "2. `softmax(x, axis=-1)` — numerically stable along given axis.\n",
    "3. `cross_entropy(pred_probs, targets)` — mean CE for one-hot targets.\n",
    "4. `positional_encoding(max_len, d_model)` — sinusoidal PE matrix [max_len, d_model].\n",
    "5. `scaled_dot_attention(q, k, v, mask=None)` — compute attention: softmax(q k^T / sqrt(d)) v with optional boolean mask (True for mask positions to -inf).\n",
    "6. `layer_norm(x, eps=1e-5)` — per-row normalization.\n",
    "7. `gru_cell(x_t, h_prev, Wx, Wh, b)` — single-step GRU: return h_t.\n"
   ],
   "id": "11b5a7883a94dc19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    x = np.asarray(x)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = np.asarray(x, float)\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_entropy(pred_probs, targets):\n",
    "    p = np.asarray(pred_probs, float)\n",
    "    y = np.asarray(targets, float)\n",
    "    eps = 1e-12\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    return float(-(y * np.log(p)).sum(axis=1).mean())\n",
    "\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos = np.arange(max_len)[:, None]\n",
    "    i = np.arange(d_model)[None, :]\n",
    "    angle_rates = 1 / np.power(10000, (2*(i//2))/d_model)\n",
    "    angles = pos * angle_rates\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return pe\n",
    "\n",
    "def scaled_dot_attention(q, k, v, mask=None):\n",
    "    q, k, v = map(lambda a: np.asarray(a, float), (q,k,v))\n",
    "    d = q.shape[-1]\n",
    "    scores = (q @ k.T) / np.sqrt(d)\n",
    "    if mask is not None:\n",
    "        m = np.asarray(mask, bool)\n",
    "        scores = np.where(m, -1e9, scores)\n",
    "    probs = softmax(scores, axis=-1)\n",
    "    return probs @ v\n",
    "\n",
    "def layer_norm(x, eps=1e-5):\n",
    "    x = np.asarray(x, float)\n",
    "    mu = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    return (x - mu) / np.sqrt(var + eps)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def gru_cell(x_t, h_prev, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Wx: weights for x -> [input_dim, 3*hidden]\n",
    "    Wh: weights for h -> [hidden, 3*hidden]\n",
    "    b: bias [3*hidden]\n",
    "    \"\"\"\n",
    "    x_t = np.asarray(x_t, float)\n",
    "    h_prev = np.asarray(h_prev, float)\n",
    "    z = x_t @ Wx + h_prev @ Wh + b\n",
    "    H = h_prev.shape[-1]\n",
    "    zt = sigmoid(z[..., :H])\n",
    "    rt = sigmoid(z[..., H:2*H])\n",
    "    ht_tilde = np.tanh(x_t @ Wx[:, 2*H:] + (rt * h_prev) @ Wh[:, 2*H:] + b[2*H:])\n",
    "    h_t = (1 - zt) * h_prev + zt * ht_tilde\n",
    "    return h_t\n"
   ],
   "id": "92dc095a4a4a8d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Asserts\n",
    "assert np.all(relu([-1,0,2]) == np.array([0,0,2]))\n",
    "\n",
    "logits = np.array([[1.0, 2.0, 3.0]])\n",
    "probs = softmax(logits, axis=-1)\n",
    "assert np.allclose(probs.sum(), 1.0)\n",
    "\n",
    "p = np.array([[0.2,0.8]])\n",
    "y = np.array([[0,1]])\n",
    "ce = cross_entropy(p,y)\n",
    "assert ce > 0\n",
    "\n",
    "pe = positional_encoding(4, 6)\n",
    "assert pe.shape == (4,6)\n",
    "\n",
    "q = np.array([[1.,0.]])\n",
    "k = np.array([[1.,0.],[0.,1.]])\n",
    "v = np.array([[1.,2.],[3.,4.]])\n",
    "out = scaled_dot_attention(q,k,v)\n",
    "assert out.shape == (1,2)\n",
    "\n",
    "ln = layer_norm(np.array([[1.,2.,3.]]))\n",
    "assert np.allclose(ln.mean(), 0, atol=1e-6)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "inp, hid = 5, 4\n",
    "Wx = rng.normal(size=(inp, 3*hid))\n",
    "Wh = rng.normal(size=(hid, 3*hid))\n",
    "b = rng.normal(size=(3*hid,))\n",
    "h0 = np.zeros(hid)\n",
    "x0 = rng.normal(size=(inp,))\n",
    "h1 = gru_cell(x0, h0, Wx, Wh, b)\n",
    "assert h1.shape == (hid,)\n",
    "\n",
    "print('Deep-Learning asserts passed ✅')\n"
   ],
   "id": "5608ddb2e9bb633a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Solutions\n",
    "\n",
    "### Theory (sample)\n",
    "1. Train tracks fit; val estimates generalization; divergence suggests overfitting or data shift.\n",
    "2. (c) ReLU\n",
    "3. Gradients propagate via chain rule from loss to weights updating by optimizer.\n",
    "4. Regularization (dropout, weight decay), data augmentation, early stopping.\n",
    "5. (c) values\n",
    "6. LSTM/GRU add gates to mitigate vanishing gradients vs vanilla RNN.\n",
    "7. Injects sequence order via sin/cos patterns enabling attention to use positions.\n",
    "8. Feeding ground-truth tokens; downside: exposure bias at inference.\n",
    "9. (a)\n",
    "10. Parameter sharing reduces parameters and exploits locality.\n",
    "\n",
    "### Fill blanks\n",
    "1. gradient (∂L/∂w)\n",
    "2. training\n",
    "3. softmax\n",
    "4. forget\n",
    "5. attention\n",
    "6. norm (value)\n",
    "7. kernel/filter\n",
    "8. feature\n"
   ],
   "id": "49afea33dff0c229"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
