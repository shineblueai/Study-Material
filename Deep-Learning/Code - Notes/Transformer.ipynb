{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step-1 : Importing Libraries",
   "id": "315fd59844bc1486"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np"
   ],
   "id": "989a27e37f4063c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 2: Implement Positional Encoding",
   "id": "c76bd4754a4ca8a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Adds positional information to token embeddings (since Transformers have no recurrence).\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Length of the input sequence (e.g., 100 tokens)\n",
    "        d_model (int): Embedding dimension (e.g., 128)\n",
    "\n",
    "    Returns:\n",
    "        pos_encoding (Tensor): Shape (1, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    positions = np.arange(seq_len)[:, np.newaxis]  # (seq_len, 1)\n",
    "    i = np.arange(d_model)[np.newaxis, :]         # (1, d_model)\n",
    "\n",
    "    # Compute angles for sine/cosine\n",
    "    angles = positions / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "\n",
    "    # Apply sin to even indices, cos to odd indices\n",
    "    pos_encoding = np.zeros_like(angles)\n",
    "    pos_encoding[:, 0::2] = np.sin(angles[:, 0::2])  # even\n",
    "    pos_encoding[:, 1::2] = np.cos(angles[:, 1::2])  # odd\n",
    "\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)  # (1, seq_len, d_model)"
   ],
   "id": "fa337de733316427"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 3: Build Multi-Head Self-Attention Layer",
   "id": "8b62176c39668968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        \"\"\"Scaled dot-product attention.\"\"\"\n",
    "        score = tf.matmul(query, key, transpose_b=True)  # (batch, heads, seq, seq)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)  # (batch, heads, seq, depth)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        \"\"\"Split embedding dim into num_heads.\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch, heads, seq, depth)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # Linear projections\n",
    "        query = self.query_dense(inputs)  # (batch, seq, embed_dim)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # Split into heads\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        # Apply attention\n",
    "        attention_output, _ = self.attention(query, key, value)\n",
    "\n",
    "        # Recombine heads\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.embed_dim))\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ],
   "id": "920685ac374045cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 4: Build Transformer Encoder Block",
   "id": "d4480b1e536570ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Self-attention + residual connection\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        # Feed-forward + residual connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ],
   "id": "5feab0d2ecc78e7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 5: Assemble Full Transformer Model",
   "id": "a55d056255169683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerModel(Model):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, max_seq_len, num_classes, num_blocks=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Embedding + positional encoding\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_enc = get_positional_encoding(max_seq_len, embed_dim)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_blocks)\n",
    "        ]\n",
    "\n",
    "        # Classifier head\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        self.dropout = layers.Dropout(0.1)\n",
    "        self.classifier = layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Embed tokens\n",
    "        x = self.token_emb(inputs)  # (batch, seq_len, embed_dim)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))  # Scale embeddings\n",
    "\n",
    "        # Add positional encoding\n",
    "        x += self.pos_enc[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "        # Apply Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, training)\n",
    "\n",
    "        # Pool and classify\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.classifier(x)"
   ],
   "id": "2b12dc4c160c3a4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 6: Instantiate and Compile Model",
   "id": "e962afa09a714164"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "VOCAB_SIZE = 10000    # e.g., unique words in your logs/docs\n",
    "MAX_SEQ_LEN = 100     # Max tokens per sequence\n",
    "EMBED_DIM = 128       # Embedding size\n",
    "NUM_HEADS = 8         # Attention heads\n",
    "FF_DIM = 128          # Feed-forward hidden size\n",
    "NUM_CLASSES = 2       # e.g., [normal, anomaly]\n",
    "\n",
    "# Create model\n",
    "model = TransformerModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_blocks=2\n",
    ")\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "f8668a992663717a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
