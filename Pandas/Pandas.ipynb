{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pandas Commands â€” One per line with purpose\n",
    "This notebook-style script lists commonly used pandas commands, one per line, each with an inline comment describing its purpose. Cells follow the same `#%% md` and `#%%` structure as other notebooks in this project.\n"
   ],
   "id": "5fe54329c36f8d62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup: imports and sample data",
   "id": "b34c281843027d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd  # import pandas main namespace\n",
    "import numpy as np   # helpful for numerical examples\n",
    "\n",
    "# Sample data to use with commands\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10.5, 20.1, np.nan, 40.2, 50.3],\n",
    "    'C': ['x', 'y', 'y', 'x', 'z'],\n",
    "    'D': pd.date_range('2025-01-01', periods=5, freq='D')\n",
    "})  # create a sample DataFrame for demonstrations\n",
    "\n",
    "s = pd.Series([1, 2, 3, 4], name='S')  # create a sample Series\n"
   ],
   "id": "6d348e35b522d0ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creation",
   "id": "2f60f3ce7ea88a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.Series([1, 2, 3])  # create a Series from a list\n",
    "pd.Series({'a': 1, 'b': 2})  # create a Series from a dict (keys become index)\n",
    "pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})  # create a DataFrame from dict of lists\n",
    "pd.DataFrame(np.arange(6).reshape(3, 2), columns=['X', 'Y'])  # create a DataFrame from a NumPy array\n",
    "pd.date_range('2025-01-01', periods=7, freq='D')  # generate a date index for time series\n",
    "pd.Categorical(['a', 'b', 'a'])  # create a categorical vector\n"
   ],
   "id": "22c4e4b5342bfd7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspection and basic info",
   "id": "3f9bd9112d4b0d66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.head()  # first 5 rows preview\n",
    "df.tail(3)  # last N rows preview\n",
    "df.sample(2, random_state=0)  # random sample of rows\n",
    "df.shape  # tuple of (rows, columns)\n",
    "df.columns  # column labels\n",
    "df.index  # row index\n",
    "df.dtypes  # data types of columns\n",
    "df.info()  # concise summary of DataFrame\n",
    "df.describe()  # descriptive statistics for numeric columns\n",
    "df.describe(include='all')  # descriptive stats including non-numeric\n",
    "df.memory_usage(deep=True)  # memory usage by column\n",
    "df.nunique()  # count of distinct values per column\n"
   ],
   "id": "656f13e634ead632"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Selection, indexing, assignment",
   "id": "b29aba916e2dd63a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['A']  # select a single column as Series\n",
    "df[['A', 'B']]  # select multiple columns as DataFrame\n",
    "df.A  # attribute-style column access (not recommended if name conflicts)\n",
    "df.loc[0]  # label-based row selection (single row by label)\n",
    "df.iloc[0]  # position-based row selection (single row by position)\n",
    "df.loc[0, 'B']  # label-based scalar access (row label, column label)\n",
    "df.iloc[0, 1]  # position-based scalar access (row pos, col pos)\n",
    "df.loc[1:3, ['A', 'C']]  # slice rows by label and select specific columns\n",
    "df.iloc[1:4, 0:2]  # slice rows and columns by position\n",
    "df.at[0, 'A']  # fast scalar access by label\n",
    "df.iat[0, 0]  # fast scalar access by position\n",
    "df.assign(E=df['A'] * 2)  # return new DataFrame with added/modified column\n",
    "df['E'] = df['A'] + 100  # add/overwrite a column in place\n",
    "df.insert(1, 'A_times2', df['A'] * 2)  # insert a column at given position\n",
    "df.rename(columns={'A': 'A_renamed'})  # return new DataFrame with renamed columns\n",
    "df.rename(columns={'A_renamed': 'A'}, inplace=False)  # demonstrate rename without changing original\n",
    "df.set_index('C')  # return new DataFrame with a column set as index\n",
    "df.reset_index()  # return new DataFrame with index turned into a column\n",
    "df.reindex(range(7))  # reindex to new labels (introduces NaNs for missing rows)\n",
    "df.filter(like='A')  # keep columns whose names contain substring\n",
    "df.get('B')  # safely get a column (returns None if missing unless default provided)\n"
   ],
   "id": "bd5ba990ce685b76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Boolean filtering and conditional ops",
   "id": "7b0a8df0f02133b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[df['A'] > 2]  # filter rows where condition holds\n",
    "df.query('A > 2 and C != \"y\"')  # query using expression string\n",
    "df[df['B'].between(10, 40, inclusive='both')]  # filter values within range\n",
    "df['C'].isin(['x', 'z'])  # membership test per element\n",
    "df.where(df['A'] % 2 == 0)  # keep values where condition True, else set NaN\n",
    "df.mask(df['A'] % 2 == 0)  # set values to NaN where condition True\n",
    "np.where(df['A'] > 3, 'high', 'low')  # vectorized conditional selection via NumPy\n"
   ],
   "id": "7ff1ce24fdaa406c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Missing data handling",
   "id": "e3b2e657cd1fcd7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.isna()  # boolean mask of missing values (NaN/NaT)\n",
    "df.notna()  # boolean mask of non-missing values\n",
    "df['B'].fillna(0)  # fill missing values with scalar\n",
    "df['B'].fillna(method='ffill')  # forward-fill missing values\n",
    "df['B'].fillna(df['B'].mean())  # fill missing with column statistic\n",
    "df.dropna()  # drop rows with any missing values\n",
    "df.dropna(subset=['B'])  # drop rows where specific column is missing\n",
    "df.interpolate()  # interpolate numeric missing values\n"
   ],
   "id": "1482c0e36402945b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sorting and ranking",
   "id": "eca157540fbf8d71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.sort_values(by='B', ascending=True)  # sort rows by column values\n",
    "df.sort_index()  # sort rows by index labels\n",
    "df['A'].rank()  # rank values with average method by default\n"
   ],
   "id": "9d58a9329d4a27c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Value exploration",
   "id": "34c65f2efb01adb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['C'].value_counts()  # frequency counts of unique values\n",
    "df['C'].value_counts(normalize=True)  # relative frequencies\n",
    "df['C'].unique()  # unique values as ndarray\n",
    "df['C'].nunique(dropna=True)  # number of unique values\n"
   ],
   "id": "16d6c59cd08c955c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Arithmetic, broadcasting, map/apply/transform",
   "id": "b0d7ce59ae7ecb8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.select_dtypes(include=np.number) + 1  # add scalar to numeric columns (broadcast)\n",
    "df.add(10)  # add scalar to all numeric columns using DataFrame method\n",
    "df.sub(df['A'], axis=0)  # subtract Series aligning on index\n",
    "df.mul(2)  # multiply all numeric columns by 2\n",
    "df.div(2)  # divide all numeric columns by 2\n",
    "df.pow(2)  # element-wise power for numeric columns\n",
    "df.apply(np.mean)  # apply function column-wise by default\n",
    "df.apply(np.mean, axis=1)  # apply function row-wise\n",
    "df.transform(lambda col: col.fillna(col.mean()))  # transform columns preserving shape\n",
    "df['A'].map({1: 'one', 2: 'two'})  # map values of a Series via dict\n",
    "df['A'].apply(lambda x: x * 10)  # apply a function element-wise on a Series\n",
    "df.clip(lower=0)  # trim values below/above thresholds\n",
    "df.round(1)  # round numeric columns to N decimals\n",
    "df.corr(numeric_only=True)  # pairwise correlation of numeric columns\n",
    "df.cov(numeric_only=True)  # covariance matrix of numeric columns\n"
   ],
   "id": "52511113185ec2b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GroupBy and aggregation",
   "id": "9a25a8c90c9510dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.groupby('C')['A'].sum()  # sum of A grouped by C\n",
    "df.groupby('C', as_index=False)['A'].mean()  # mean as a DataFrame with group labels as columns\n",
    "df.groupby(['C'])['A'].agg(['mean', 'sum', 'count'])  # multiple aggregations per group\n",
    "df.groupby('C').agg(A_mean=('A', 'mean'), B_max=('B', 'max'))  # named aggregations\n",
    "df.groupby('C').transform(lambda x: x.fillna(x.mean()))  # transform to broadcast back to original shape\n",
    "df.groupby('C').apply(lambda g: g.head(1))  # apply custom function to each group\n"
   ],
   "id": "e029ee91c9b34bea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge, join, concatenate",
   "id": "5860c94c32313930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "left = pd.DataFrame({'key': ['k1','k2','k3'], 'L': [1,2,3]})  # sample left table for joins\n",
    "right = pd.DataFrame({'key': ['k1','k2','k4'], 'R': [10,20,40]})  # sample right table for joins\n",
    "pd.merge(left, right, on='key', how='inner')  # database-style join on key columns\n",
    "pd.merge(left, right, on='key', how='left')  # left join keeps all left rows\n",
    "pd.merge(left, right, on='key', how='outer')  # outer join keeps union of keys\n",
    "left.join(right.set_index('key'), on='key')  # join by index of right\n",
    "pd.concat([left, right], axis=0, ignore_index=True)  # stack rows from multiple DataFrames\n",
    "pd.concat([left, right], axis=1)  # concatenate side-by-side by index\n"
   ],
   "id": "cfd1eef90bc2c938"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reshaping: pivot, melt, stack/unstack, crosstab",
   "id": "4f20e8f4408616a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "wide = pd.DataFrame({'id':[1,1,2,2],'var':['v1','v2','v1','v2'],'val':[10,20,30,40]})  # sample long table\n",
    "wide.pivot(index='id', columns='var', values='val')  # make columns out of unique values\n",
    "pd.pivot_table(wide, index='id', columns='var', values='val', aggfunc='mean')  # pivot with aggregation\n",
    "pd.melt(wide.pivot(index='id', columns='var', values='val').reset_index(), id_vars='id', var_name='var', value_name='val')  # unpivot columns into rows\n",
    "wide.set_index(['id','var']).unstack('var')  # reshape with MultiIndex to columns\n",
    "pd.crosstab(wide['id'], wide['var'])  # contingency table of counts\n"
   ],
   "id": "fa0146046ebfb498"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Window operations",
   "id": "4565965db216d0e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['A'].rolling(window=3).mean()  # moving average over rolling window\n",
    "df['A'].expanding().sum()  # cumulative sum with expanding window\n",
    "df['A'].ewm(alpha=0.3).mean()  # exponentially weighted moving average\n"
   ],
   "id": "45e4f81250c1a354"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Date and time handling",
   "id": "bf0c55dbb92354c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.to_datetime(['2025-01-01', '2025-01-05'])  # convert strings to datetime dtype\n",
    "pd.date_range(start='2025-01-01', end='2025-01-10', freq='2D')  # create a date range with frequency\n",
    "pd.period_range('2025-01', periods=3, freq='M')  # create a period range (monthly)\n",
    "df['D'].dt.year  # extract year from datetime column\n",
    "df['D'].dt.month  # extract month from datetime column\n",
    "df['D'].dt.day_name()  # get weekday name for each date\n",
    "df.set_index('D').resample('2D').mean(numeric_only=True)  # resample time series by frequency\n"
   ],
   "id": "ec3d1f5e38576658"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Categorical data",
   "id": "2f6b4a2048534a11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['C'].astype('category')  # convert to categorical dtype\n",
    "df['C'].astype('category').cat.categories  # list distinct categories\n",
    "df['C'].astype('category').cat.codes  # integer codes for categories\n",
    "df['C'].astype('category').cat.add_categories(['new'])  # add new categories\n"
   ],
   "id": "95dee16f3a235164"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# String methods (vectorized)",
   "id": "9da751999c02f662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['C'].str.upper()  # uppercase strings\n",
    "df['C'].str.contains('x')  # check substring presence per element\n",
    "df['C'].str.replace('x', 'X', regex=False)  # replace substring (non-regex)\n",
    "df['C'].str.len()  # length of each string\n",
    "df['C'].str.slice(0, 1)  # slice each string\n"
   ],
   "id": "a0ee010b3859faf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Duplicates, reshuffling, and utilities",
   "id": "5d3c3811282f3cf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.duplicated()  # boolean mask of duplicated rows\n",
    "df.drop_duplicates()  # remove duplicate rows\n",
    "df.sample(frac=0.4, random_state=42)  # random fraction of rows\n",
    "df.take([0, 2, 4])  # take rows by positional indices\n",
    "df.pipe(lambda _df: _df.head(2))  # functional-style method chaining via pipe\n"
   ],
   "id": "7fa5a938b8c4469e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Type conversion and replacements",
   "id": "5448fdcfb3b5a3c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.astype({'A': 'float64'})  # convert column(s) to specific dtype\n",
    "pd.to_numeric(pd.Series(['1', '2', 'x']), errors='coerce')  # convert to numeric, set invalid parsing as NaN\n",
    "df.replace({np.nan: -1})  # replace values, including NaN handling\n",
    "df.clip(lower=0, upper=100)  # constrain values within bounds\n"
   ],
   "id": "155283990ab60679"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation and expressions",
   "id": "2237110f1e5890be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.eval('A_plus_B = A + B')  # evaluate string expressions to create/modify columns\n",
    "df.query('A % 2 == 1')  # select rows using expression language\n"
   ],
   "id": "b825ceed6970792e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MultiIndex operations",
   "id": "68d4a4c31d126c9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mi = pd.MultiIndex.from_product([['g1','g2'], ['x','y']], names=['grp','key'])  # create a MultiIndex\n",
    "mdf = pd.DataFrame({'val':[1,2,3,4]}, index=mi)  # DataFrame with MultiIndex\n",
    "mdf.unstack('key')  # move specified level from index to columns\n",
    "mdf.stack()  # inverse of unstack: move columns back into index\n",
    "mdf.swaplevel('grp','key')  # swap two index levels\n",
    "mdf.sort_index(level='grp')  # sort by a specific index level\n"
   ],
   "id": "1c84bdc4313edf33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Input/Output (examples)\n",
    "# Note: I/O commands are shown as examples; uncomment and adjust paths before using."
   ],
   "id": "2e3ff4b215a3fc53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pd.read_csv('path/to/file.csv')  # read CSV from file\n",
    "# pd.read_excel('path/to/file.xlsx', sheet_name=0)  # read Excel file\n",
    "# pd.read_json('path/to/file.json')  # read JSON file\n",
    "# pd.read_parquet('path/to/file.parquet')  # read Parquet file (requires pyarrow or fastparquet)\n",
    "# pd.read_feather('path/to/file.feather')  # read Feather file\n",
    "# pd.read_pickle('path/to/file.pkl')  # read Python pickle\n",
    "# pd.read_html('https://example.com/tablepage')  # read HTML tables into list of DataFrames\n",
    "# pd.read_sql('SELECT * FROM table', con=some_sqlalchemy_engine)  # read from SQL query\n",
    "\n",
    "# df.to_csv('out.csv', index=False)  # write DataFrame to CSV\n",
    "# df.to_excel('out.xlsx', index=False)  # write DataFrame to Excel\n",
    "# df.to_json('out.json', orient='records')  # write DataFrame to JSON\n",
    "# df.to_parquet('out.parquet', index=False)  # write DataFrame to Parquet\n",
    "# df.to_feather('out.feather')  # write DataFrame to Feather\n",
    "# df.to_pickle('out.pkl')  # write DataFrame to pickle\n"
   ],
   "id": "c5d6c1c2359aa989"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotting (requires matplotlib)",
   "id": "ad563014f35cba87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.plot(y='B', kind='line')  # quick line plot of column B\n",
    "df.plot.scatter(x='A', y='B')  # scatter plot of A vs B\n",
    "df['A'].plot.hist(bins=5)  # histogram of A\n"
   ],
   "id": "95477c3a79229a88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Options and display settings",
   "id": "87de9d02bbc7afd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.set_option('display.max_rows', 100)  # set max rows to display\n",
    "pd.set_option('display.max_columns', 20)  # set max columns to display\n",
    "pd.reset_option('display.max_rows')  # reset an option to default\n",
    "pd.options.display.float_format = '{:,.2f}'.format  # set global float display format\n"
   ],
   "id": "fb8beb59336aa3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Performance and memory tips",
   "id": "e77ed6220ba6b1e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.astype({'A': 'int32', 'B': 'float32'})  # downcast dtypes to reduce memory\n",
    "df['C'].astype('category')  # use categorical dtype for repeated strings\n",
    "pd.eval('x + y', engine='numexpr', parser='pandas', target=None, local_dict={'x': df['A'], 'y': df['A']})  # fast eval of expressions\n",
    "df.memory_usage(deep=True).sum()  # total memory usage in bytes\n"
   ],
   "id": "aa6520de6f11b6f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
