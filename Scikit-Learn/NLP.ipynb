{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Natural Language Processing (NLP) — One command per code cell with purpose\n",
    "This notebook-style script lists commonly used NLP commands across popular libraries (NLTK, spaCy, TextBlob, scikit-learn, Gensim, Transformers, Sentence-Transformers). Each code cell contains one primary command with an inline comment describing its purpose. Heavy downloads and network I/O are commented.\n"
   ],
   "id": "a81ef9ebfaaf53d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation (commented) — run in your environment if needed\n",
    "# - Only install what you need"
   ],
   "id": "e6884481e6616885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install nltk spacy textblob gensim scikit-learn transformers sentence-transformers spacy-lookups-data\n",
   "id": "1c12e6cb841fef17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optional model/corpus downloads (commented)\n",
    "# - NLTK data: tokenizers, stopwords, wordnet\n",
    "# - spaCy model: small English model for POS/NER/lemma"
   ],
   "id": "dc64b1c46d10a2b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('averaged_perceptron_tagger'); nltk.download('maxent_ne_chunker'); nltk.download('words')  # comment/uncomment per need\n",
    "# !python -m spacy download en_core_web_sm  # download spaCy English model (commented)\n"
   ],
   "id": "e030b84d24416f21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup: imports and sample texts",
   "id": "7d8977dc8acfc4fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import re  # regex utilities for simple text cleanup\n",
   "id": "b40acfcb3c4e70c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from pprint import pprint  # pretty printing helper for small outputs\n",
   "id": "1e3d5bbf1d2cfb12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_text = \"\"\"Natural Language Processing (NLP) enables computers to understand human language.\\n\n",
    "It includes tokenization, stopwords removal, stemming, lemmatization, POS tagging, and NER.\\n\n",
    "Apple is looking at buying U.K. startup for $1 billion.\\n\n",
    "SpaCy and NLTK are common Python libraries for NLP!\"\"\"  # small multi-line text for demos\n"
   ],
   "id": "e4701313764af94a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NLTK — imports",
   "id": "5eb0049b2b12b664"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import nltk  # main NLTK package for classic NLP tasks\n",
   "id": "ef598fd2cd3594ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from nltk.tokenize import word_tokenize, sent_tokenize  # tokenizers for words and sentences\n",
   "id": "d2553f959e9ba8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from nltk.corpus import stopwords  # stop words list (requires nltk.download('stopwords'))\n",
   "id": "e1ba566044c7426e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer  # stemming and lemmatization tools\n",
   "id": "469ddff37f5a1bd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenization",
   "id": "fd03fbdcd8089f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sent_tokenize(sample_text)  # split text into sentences\n",
   "id": "906c0cd0aebe70a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "word_tokenize(sample_text)  # split text into word tokens (punctuation as separate tokens)\n",
   "id": "9b7de2aa9c3822e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stop words",
   "id": "42bfca69706bfac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "stopwords.words('english')  # get built-in English stop words list\n",
   "id": "ec9ef4aa449b68d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokens = word_tokenize(sample_text)  # tokenize once to reuse in later cells\n",
   "id": "eec4be6fc8625db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "{w.lower() for w in tokens if w.isalpha()}  # unique alphabetic tokens lowercased (simple normalization)\n",
   "id": "ae653cd6bed59612"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "{w for w in tokens if w.lower() not in set(stopwords.words('english')) and w.isalpha()}  # remove stop words (toy example)\n",
   "id": "7de57505cf14dd38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stemming and Lemmatization",
   "id": "de29aa5c7c1ae792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "PorterStemmer().stem('running')  # reduce word to its stem/root via Porter stemmer\n",
   "id": "16731cd321c2fd7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "SnowballStemmer('english').stem('studies')  # stem using Snowball (aka Porter2) stemmer\n",
   "id": "238c2c6dd2c32b5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "WordNetLemmatizer().lemmatize('better', pos='a')  # lemmatize (needs WordNet; pos helps with correct lemma)\n",
   "id": "c9eb5654310341f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[PorterStemmer().stem(w) for w in tokens if w.isalpha()]  # stem a list of tokens (demo list comprehension)\n",
   "id": "e95e4633bac74007"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[WordNetLemmatizer().lemmatize(w) for w in tokens if w.isalpha()]  # lemmatize a list of tokens\n",
   "id": "159595086171deb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# N-grams and POS/NER",
   "id": "31af212e9900a7d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "list(nltk.ngrams([w.lower() for w in tokens if w.isalpha()], 2))  # generate bigrams over alphabetic tokens\n",
   "id": "2851e62749ea438e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nltk.pos_tag(tokens)  # part-of-speech tagging for tokens (requires tagger data)\n",
   "id": "51f29a831ea7696"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nltk.ne_chunk(nltk.pos_tag(word_tokenize(\"Apple is buying a startup in U.K.\")))  # NLTK named entity chunking (tree output)\n",
   "id": "3b154a2f80a702b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text normalization helpers (regex)",
   "id": "c9a9bbc02019282b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "re.sub(r\"[^\\w\\s]\", \"\", sample_text.lower())  # lowercase and strip punctuation (very simple cleaning)\n",
   "id": "c22f38c748a51021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TextBlob — quick sentiment, tokens, noun phrases",
   "id": "25a2dad39e0b427e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from textblob import TextBlob  # high-level text processing library\n",
   "id": "d906a6f222f6348f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TextBlob(sample_text).sentiment  # polarity (-1..1) and subjectivity (0..1)\n",
   "id": "c7f4240864ee822c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TextBlob(\"I absolutely love natural language processing!\").sentences  # sentence objects from text\n",
   "id": "e5fbf6089c083977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TextBlob(\"Cats are running and ate fishes\").correct()  # spelling correction (toy) — may be slow on large text\n",
   "id": "1eb6fbddb6540296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TextBlob(\"New York City is great.\").noun_phrases  # noun phrase extraction\n",
   "id": "3d99921a3517ef53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TextBlob(\"Bonjour le monde\").translate(to='en')  # translation (uses web APIs; commented to avoid network)\n",
   "id": "5955a8c7148c5ec1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# spaCy — models and processing\n",
    "# Note: Accurate POS/NER requires a trained model (e.g., en_core_web_sm). We'll show a blank pipeline as safe default."
   ],
   "id": "3868ccc47cd6359f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import spacy  # industrial-strength NLP library\n",
   "id": "9087e78099a06b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nlp = spacy.blank('en')  # create a blank English pipeline (no trained components)\n",
   "id": "526017e80098a844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# nlp = spacy.load('en_core_web_sm')  # load small English model for POS/NER/lemma (uncomment after installing)\n",
   "id": "a9b6eb90a0ef3e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")  # process text to a Doc object\n",
   "id": "9984350b7548be10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[t.text for t in doc]  # token texts from spaCy Doc\n",
   "id": "4cd9e31f06ffa22a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "getattr(doc[0], 'lemma_', '')  # access lemma of a token (empty if component not available)\n",
   "id": "bfcaa0520acafc76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[(ent.text, ent.label_) for ent in doc.ents]  # named entities (empty for blank model)\n",
   "id": "d24e3baccf785794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[(t.text, t.pos_) for t in doc]  # part-of-speech tags (empty for blank model)\n",
   "id": "23308efa66168f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[chunk.text for chunk in getattr(doc, 'noun_chunks', [])]  # noun chunks (requires parser in loaded model)\n",
   "id": "222b288a573cc91c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# scikit-learn — Bag of Words (CountVectorizer) and TF-IDF (TfidfVectorizer)",
   "id": "ce6f15d83c984a05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # classic vectorizers for text\n",
   "id": "c477848188b5b910"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "corpus = [\n",
    "    \"NLP enables computers to understand language\",\n",
    "    \"Language models perform tokenization and lemmatization\",\n",
    "    \"Named entity recognition is a common NLP task\",\n",
    "]  # tiny corpus of 3 documents\n"
   ],
   "id": "7ccc12c1049d5418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "CountVectorizer(max_features=20).fit_transform(corpus).toarray()  # bag-of-words document-term matrix\n",
   "id": "99f63c330af6e8bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TfidfVectorizer(ngram_range=(1,2), stop_words='english').fit_transform(corpus).toarray()  # TF-IDF with unigrams+bigrams\n",
   "id": "2a04c0b946fc9f31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "CountVectorizer(min_df=1, max_df=1.0, binary=True).fit(corpus).get_feature_names_out()  # learned vocabulary terms\n",
   "id": "16cfa8fe07c48145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tfidf = TfidfVectorizer().fit_transform(corpus)  # compute TF-IDF matrix (sparse)\n",
   "id": "1d95f3f310bc9bd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.metrics.pairwise import cosine_similarity  # pairwise similarity for vectors\n",
   "id": "51cb53f9952ef531"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cosine_similarity(tfidf)  # document-to-document cosine similarity matrix using TF-IDF features\n",
   "id": "f77a866e945bfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Similarity search (scikit-learn) — NearestNeighbors with cosine distance",
   "id": "5a11b651299accff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.neighbors import NearestNeighbors  # k-NN search over vector spaces\n",
   "id": "ffa38e6866322802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nn = NearestNeighbors(metric='cosine', n_neighbors=2).fit(tfidf)  # fit nearest neighbors index on TF-IDF vectors\n",
   "id": "5fdc0e01e14cca9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nn.kneighbors(TfidfVectorizer().fit_transform([\"NLP and language processing\"]))  # find nearest docs to a query\n",
   "id": "d08b9887d577186c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gensim — Word2Vec training and usage",
   "id": "c4305cce235d8ef6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from gensim.models import Word2Vec  # Word2Vec implementation (CBOW/Skip-gram)\n",
   "id": "3c201df91fbbe1ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "toy_sentences = [\n",
    "    [\"nlp\", \"enables\", \"computers\", \"understand\", \"language\"],\n",
    "    [\"language\", \"models\", \"perform\", \"tokenization\"],\n",
    "    [\"named\", \"entity\", \"recognition\", \"is\", \"nlp\"],\n",
    "]  # toy tokenized sentences\n"
   ],
   "id": "1f01c2ee69258352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "w2v = Word2Vec(toy_sentences, vector_size=50, window=2, min_count=1, workers=1, sg=1, seed=42)  # train a tiny Word2Vec model\n",
   "id": "4bc0476744054016"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "w2v.wv.most_similar(\"nlp\")  # query most similar words to a given token in the trained space\n",
   "id": "a396662834d677d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GloVe vectors (pretrained) — loading with Gensim KeyedVectors (commented for safety)",
   "id": "9eb7e6e2dc3dae55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# # Convert GloVe to word2vec format once, or use ready-made files\n",
    "# # kv = KeyedVectors.load_word2vec_format('glove.6B.50d.word2vec.txt', binary=False)  # path required\n",
    "# # kv.most_similar('king')  # example lookup (commented)\n"
   ],
   "id": "740878c136291140"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformers — tokenizers and embeddings (commented to avoid downloads)",
   "id": "13c6e4c2d4764a7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# tok = AutoTokenizer.from_pretrained('distilbert-base-uncased')  # load tokenizer (downloads model files)\n"
   ],
   "id": "d1eae712dbf4d070"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# mdl = AutoModel.from_pretrained('distilbert-base-uncased')  # load transformer model (downloads weights)\n",
   "id": "9716969e1e6295ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# enc = tok(\"NLP with BERT embeddings\", return_tensors='pt')  # tokenize text to model inputs (PyTorch tensors)\n",
   "id": "b98fc135be868b15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# mdl(**enc).last_hidden_state.mean(dim=1)  # pooled embedding by averaging last hidden states (toy example)\n",
   "id": "8f5ffca174b7b3b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformers — feature extraction pipeline (commented)",
   "id": "c1499583882be964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from transformers import pipeline\n",
    "# fe = pipeline('feature-extraction', model='distilbert-base-uncased')  # build feature extraction pipeline\n"
   ],
   "id": "36241472de478453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# fe(\"Semantic embeddings are useful for search\")  # extract per-token embeddings (list of vectors)\n",
   "id": "162a3d9d372f082d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentence-Transformers — sentence embeddings and semantic similarity\n",
    "# Note: Downloads a small model on first use; uncomment to run."
   ],
   "id": "c156b821b843671d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# sbert = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # compact, fast sentence embedding model\n"
   ],
   "id": "4faeb7e821ce4266"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# emb = sbert.encode([\"NLP is great\", \"I love language processing\", \"Cats and dogs\"], normalize_embeddings=True)  # encode texts\n",
   "id": "79caef266f9f4f74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# util.cos_sim(emb, emb)  # cosine similarity matrix between sentences\n",
   "id": "438ee7fec9c4d676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# util.semantic_search(emb[0:1], emb, top_k=2)  # semantic search: top-2 sentences most similar to the first\n",
   "id": "9c62a26806ddacdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Named Entity Recognition (spaCy model) — accurate (commented; requires model)",
   "id": "1371bd671acb3680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# nlp_sm = spacy.load('en_core_web_sm')  # load small English model\n",
   "id": "7b1d738147db2633"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# [(ent.text, ent.label_) for ent in nlp_sm(\"Apple is buying a U.K. startup for $1 billion\").ents]  # extract entities\n",
   "id": "22aef48bdfbd3cc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# End-to-end pattern: Preprocess → Vectorize → Similarity (tiny example)",
   "id": "6cc3716d9fef95e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize(txt: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", re.sub(r\"[^\\w\\s]\", \"\", txt.lower())).strip()  # simple lowercase + punctuation strip\n"
   ],
   "id": "f2095e0002e52f14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "norm_corpus = [normalize(d) for d in corpus]  # apply simple normalization to corpus\n",
   "id": "761f93ca99c6f2cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tfidf2 = TfidfVectorizer().fit_transform(norm_corpus)  # build TF-IDF on normalized corpus\n",
   "id": "2931f1b6bd7853f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cosine_similarity(tfidf2, TfidfVectorizer().fit_transform([normalize(\"NLP language models\")] ))  # similarity of query to corpus\n",
   "id": "f3e734bee9f04c19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "1. Many commands require prior downloads (NLTK corpora, spaCy models, Transformers weights). These are commented to keep this file runnable by default.\n",
    "2. For high-quality POS/NER, prefer `spacy.load('en_core_web_sm')` (or larger) over a blank pipeline.\n",
    "3. Use `NearestNeighbors(metric='cosine')` or FAISS/Annoy for scalable semantic search over embeddings.\n",
    "4. Replace toy corpora and examples with your datasets as you study.\n"
   ],
   "id": "250cc05b8b3dff8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
