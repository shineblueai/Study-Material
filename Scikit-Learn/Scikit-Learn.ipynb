{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scikit-Learn Commands — One per line with purpose\n",
    "This notebook-style script lists commonly used scikit-learn commands, one per line, each with an inline comment describing its purpose. It follows the same `#%% md` and `#%%` cell structure as the other study notebooks.\n"
   ],
   "id": "dcc176134885222e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup: imports and sample data",
   "id": "413d209e70db4c11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np  # numerical support\n",
    "from sklearn import set_config  # for display options\n",
    "from sklearn.datasets import load_iris, load_diabetes, make_classification, make_regression  # toy datasets\n",
    "from sklearn.model_selection import train_test_split  # split utilities\n",
    "\n",
    "# Small datasets for examples\n",
    "iris = load_iris()  # classic multiclass classification dataset\n",
    "X_iris, y_iris = iris.data, iris.target  # features and target\n",
    "\n",
    "X_cls, y_cls = make_classification(n_samples=200, n_features=10, n_informative=5, random_state=42)  # synthetic binary classification data\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, noise=5.0, random_state=42)  # synthetic regression data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)  # split into train/test sets\n"
   ],
   "id": "2ce5929da78e4a07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Datasets: loaders, fetchers, and generators\n",
    "# This section shows how to load small built-in datasets, fetch larger ones (may download), and generate synthetic datasets."
   ],
   "id": "92012a896bec54ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.datasets import (\n",
    "    load_wine, load_breast_cancer, load_digits, load_diabetes as _load_diabetes, load_linnerud\n",
    ")  # built-in dataset loaders (alias diabetes to avoid shadowing above variable)\n",
    "\n",
    "wine = load_wine(as_frame=True)  # load Wine dataset; returns Bunch with .data/.target/.frame\n",
    "wine.frame.head()  # view first rows as a single DataFrame when as_frame=True\n",
    "\n",
    "X_wine, y_wine = load_wine(return_X_y=True)  # get features/target directly as arrays\n",
    "breast = load_breast_cancer(as_frame=True)  # binary classification dataset (breast cancer)\n",
    "digits = load_digits()  # 8x8 image digits dataset (classification)\n",
    "diabetes_bunch = _load_diabetes(as_frame=True)  # regression dataset about diabetes progression\n",
    "linnerud = load_linnerud(as_frame=True)  # small multivariate dataset (exercise data)\n",
    "\n",
    "# Accessing metadata from Bunch objects (common pattern)\n",
    "wine.DESCR  # long-form description of the dataset\n",
    "wine.feature_names  # list of feature names\n"
   ],
   "id": "f9aadaffd2efa9fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset fetchers (may download data — commented out by default)\n",
    "# Uncomment the following lines in an interactive environment with internet access."
   ],
   "id": "bdf20a3293165be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from sklearn.datasets import fetch_20newsgroups, fetch_20newsgroups_vectorized  # text datasets\n",
    "# from sklearn.datasets import fetch_california_housing  # tabular regression dataset\n",
    "# from sklearn.datasets import fetch_openml, get_data_home, clear_data_home  # OpenML interface and data cache utils\n",
    "\n",
    "# fetch_20newsgroups(subset='train', categories=['sci.space', 'rec.sport.baseball'], remove=('headers', 'footers', 'quotes'))  # fetch subset of 20NG text\n",
    "# fetch_20newsgroups_vectorized(subset='train')  # pre-vectorized (tf-idf) 20NG features\n",
    "# fetch_california_housing(as_frame=True)  # California housing regression dataset as DataFrame\n",
    "# fetch_openml(name='titanic', version=1, as_frame=True)  # fetch dataset by name from OpenML (requires internet)\n",
    "# fetch_openml(data_id=61, as_frame=True)  # fetch dataset by ID from OpenML\n",
    "# get_data_home()  # show local scikit-learn data cache directory\n",
    "# clear_data_home()  # delete all cached datasets (use with caution)\n"
   ],
   "id": "2c233d7cf7c8e24d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Synthetic dataset generators (quickly create toy data)",
   "id": "403e70a129b66f61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.datasets import make_blobs, make_moons, make_circles  # synthetic data generators\n",
    "\n",
    "X_blob, y_blob = make_blobs(n_samples=200, centers=3, cluster_std=1.2, random_state=42)  # isotropic Gaussian blobs for clustering\n",
    "X_moon, y_moon = make_moons(n_samples=200, noise=0.2, random_state=42)  # two interleaving half circles (binary classification)\n",
    "X_circ, y_circ = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=42)  # concentric circles (binary classification)\n"
   ],
   "id": "e05913094ac1e76f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Global configuration and inspection",
   "id": "b9ce2ee1e2039533"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "set_config(display='diagram')  # display pipelines/estimators as diagrams in rich environments\n",
   "id": "d777a3cfd7507bb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model selection: splitting and cross-validation",
   "id": "461c4cbabf123233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold  # CV splitters\n",
    "from sklearn.model_selection import cross_val_score, cross_validate  # cross-validation helpers\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  # hyperparameter search\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, TimeSeriesSplit  # specialized splitters\n",
    "\n",
    "KFold(n_splits=5, shuffle=True, random_state=42)  # K-fold CV splitter for general tasks\n",
    "StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # preserves label distribution across folds\n",
    "GroupKFold(n_splits=3)  # ensures samples with the same group are in the same fold\n",
    "cross_val_score(estimator=None, X=X_iris, y=y_iris, scoring=None, cv=5)  # compute CV scores (estimator to be provided)\n",
    "cross_validate(estimator=None, X=X_iris, y=y_iris, scoring=['accuracy', 'f1_macro'], cv=5, return_train_score=False)  # multiple metrics CV\n",
    "GridSearchCV(estimator=None, param_grid={'C': [0.1, 1, 10]}, cv=5, n_jobs=None)  # exhaustive search over param grid\n",
    "RandomizedSearchCV(estimator=None, param_distributions={'C': [0.1, 1, 10]}, n_iter=5, cv=5, random_state=42)  # random hyperparam search\n",
    "StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)  # random stratified splits\n",
    "TimeSeriesSplit(n_splits=5)  # splitter for time-ordered data\n"
   ],
   "id": "c59bb39b40c12fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing: scaling, normalization, encoding, transforms",
   "id": "943c111ef1ab6f16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer  # scaling/normalization\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder  # categorical encoders\n",
    "from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer  # feature engineering\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, QuantileTransformer  # transformations\n",
    "\n",
    "StandardScaler()  # standardize features (zero mean, unit variance)\n",
    "MinMaxScaler()  # scale features to a given range (default [0,1])\n",
    "RobustScaler()  # scale using statistics robust to outliers (median/IQR)\n",
    "Normalizer(norm='l2')  # normalize samples to unit norm\n",
    "OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # one-hot encode categorical features\n",
    "OrdinalEncoder()  # encode categories as ordered integer codes\n",
    "PolynomialFeatures(degree=2, include_bias=False)  # generate polynomial/interaction features\n",
    "KBinsDiscretizer(n_bins=5, encode='onehot', strategy='quantile')  # discretize continuous features\n",
    "FunctionTransformer(np.log1p, feature_names_out='one-to-one')  # wrap a NumPy/pandas function as a transformer\n",
    "PowerTransformer(method='yeo-johnson')  # stabilize variance and make data more Gaussian-like\n",
    "QuantileTransformer(output_distribution='normal', random_state=42)  # map features to a given distribution\n"
   ],
   "id": "ee14aab6fb0d567"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imputation for missing data",
   "id": "8213d551633d600c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer  # imputers\n",
    "\n",
    "SimpleImputer(strategy='mean')  # impute missing numeric values with mean\n",
    "KNNImputer(n_neighbors=5)  # impute using nearest neighbors\n"
   ],
   "id": "b800635b235e130c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature selection and dimensionality reduction",
   "id": "797020cfbeae1adf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif  # univariate selection\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel, VarianceThreshold  # model-based selection\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF  # decomposition methods\n",
    "\n",
    "SelectKBest(score_func=f_classif, k=10)  # select top-k features by ANOVA F-score\n",
    "SelectKBest(score_func=chi2, k=10)  # select top-k features by chi-squared (non-negative features)\n",
    "VarianceThreshold(threshold=0.0)  # remove features with low variance\n",
    "RFE(estimator=None, n_features_to_select=5)  # recursive feature elimination with base estimator\n",
    "RFECV(estimator=None, step=1, cv=5)  # RFECV selects optimal number of features via CV\n",
    "SelectFromModel(estimator=None, threshold='median')  # select features based on model importance/coefficients\n",
    "PCA(n_components=2, random_state=42)  # principal component analysis for dimensionality reduction\n",
    "TruncatedSVD(n_components=2, random_state=42)  # SVD for sparse matrices or non-centered data\n",
    "NMF(n_components=2, init='random', random_state=42)  # non-negative matrix factorization\n"
   ],
   "id": "c719dcfb12a6adc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Composition: pipelines and column-wise processing",
   "id": "5fe26c77a44f9338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline  # pipelines\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector  # column-wise transforms\n",
    "\n",
    "Pipeline(steps=[('scale', StandardScaler()), ('model', None)])  # define ordered preprocessing + model steps\n",
    "make_pipeline(StandardScaler())  # quick pipeline creation without naming steps\n",
    "ColumnTransformer(transformers=[('num', StandardScaler(), make_column_selector(dtype_include=np.number))])  # apply transforms by column type\n",
    "make_column_selector(pattern='^feat_')  # helper to select columns by regex pattern\n"
   ],
   "id": "8347f30e52aefbc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Linear models (classification/regression)",
   "id": "16ff9fe377e3f969"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet  # core linear models\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Perceptron, PassiveAggressiveClassifier  # online/large-scale\n",
    "\n",
    "LogisticRegression(max_iter=1000, random_state=42)  # multinomial/binary classifier with regularization\n",
    "LinearRegression()  # ordinary least squares regression\n",
    "Ridge(alpha=1.0, random_state=42)  # L2-regularized regression\n",
    "Lasso(alpha=0.001, random_state=42)  # L1-regularized regression (sparse coefficients)\n",
    "ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42)  # combined L1/L2 regularization\n",
    "SGDClassifier(loss='log_loss', max_iter=1000, random_state=42)  # linear classifier trained with SGD\n",
    "SGDRegressor(max_iter=1000, random_state=42)  # linear regressor trained with SGD\n",
    "Perceptron(max_iter=1000, random_state=42)  # linear binary classifier (Perceptron rule)\n",
    "PassiveAggressiveClassifier(max_iter=1000, random_state=42)  # online large-margin classifier\n"
   ],
   "id": "eb82f62e5665cfb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Support Vector Machines",
   "id": "c7a8eebd244ff265"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR  # SVM estimators\n",
    "\n",
    "SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)  # kernel SVM classifier\n",
    "SVR(kernel='rbf', C=1.0, gamma='scale')  # kernel SVM regressor\n",
    "LinearSVC(C=1.0, random_state=42)  # linear SVM classifier (efficient for large features)\n",
    "LinearSVR(C=1.0, random_state=42)  # linear SVM regressor\n"
   ],
   "id": "2a1bd005f8e62163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tree-based models and ensembles",
   "id": "676d1a3ebfd9df1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor  # decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor  # random forests\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor  # gradient boosting trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor  # extremely randomized trees\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor  # fast histogram-based GB\n",
    "\n",
    "DecisionTreeClassifier(random_state=42)  # non-linear classifier tree\n",
    "DecisionTreeRegressor(random_state=42)  # non-linear regression tree\n",
    "RandomForestClassifier(n_estimators=200, random_state=42)  # ensemble of decision trees for classification\n",
    "RandomForestRegressor(n_estimators=200, random_state=42)  # ensemble of decision trees for regression\n",
    "GradientBoostingClassifier(random_state=42)  # sequential boosting of weak learners (classification)\n",
    "GradientBoostingRegressor(random_state=42)  # sequential boosting of weak learners (regression)\n",
    "ExtraTreesClassifier(n_estimators=200, random_state=42)  # randomized tree ensemble with extra randomness\n",
    "ExtraTreesRegressor(n_estimators=200, random_state=42)  # randomized tree ensemble for regression\n",
    "HistGradientBoostingClassifier(random_state=42)  # fast histogram-based gradient boosting classifier\n",
    "HistGradientBoostingRegressor(random_state=42)  # fast histogram-based gradient boosting regressor\n"
   ],
   "id": "52bfe34ce81f9c42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neighbors and Naive Bayes",
   "id": "1a9efcda182300f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, NearestNeighbors  # neighbors-based methods\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB  # naive bayes variants\n",
    "\n",
    "KNeighborsClassifier(n_neighbors=5)  # k-NN classifier\n",
    "KNeighborsRegressor(n_neighbors=5)  # k-NN regressor\n",
    "NearestNeighbors(n_neighbors=5)  # unsupervised nearest neighbors queries\n",
    "GaussianNB()  # Gaussian naive Bayes classifier for continuous features\n",
    "MultinomialNB(alpha=1.0)  # naive Bayes for discrete counts (e.g., text)\n",
    "BernoulliNB(alpha=1.0)  # naive Bayes for binary/boolean features\n"
   ],
   "id": "8512ddbe8ab2ed6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering",
   "id": "a82b61b238830171"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering  # clustering algorithms\n",
    "\n",
    "KMeans(n_clusters=3, n_init='auto', random_state=42)  # partition data into k clusters via k-means\n",
    "DBSCAN(eps=0.5, min_samples=5)  # density-based clustering finding arbitrary-shaped clusters\n",
    "AgglomerativeClustering(n_clusters=3, linkage='ward')  # hierarchical clustering\n",
    "SpectralClustering(n_clusters=3, random_state=42)  # graph-based clustering using spectral embeddings\n"
   ],
   "id": "d356033389158365"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Manifold learning and embedding",
   "id": "d4d882046596f09d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.manifold import TSNE, Isomap  # nonlinear manifold embeddings\n",
    "\n",
    "TSNE(n_components=2, perplexity=30, random_state=42)  # t-SNE embedding for visualization (slow on large data)\n",
    "Isomap(n_neighbors=5, n_components=2)  # isometric mapping for nonlinear dimensionality reduction\n"
   ],
   "id": "7d6da0a4722a8699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calibration, probability and decision functions",
   "id": "462b36b538ae27bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV  # probability calibration\n",
    "from sklearn.preprocessing import Binarizer  # thresholding transformer\n",
    "\n",
    "CalibratedClassifierCV(base_estimator=None, cv=5, method='isotonic')  # calibrate classifier probabilities\n",
    "Binarizer(threshold=0.0)  # binarize numeric features using a threshold\n"
   ],
   "id": "920b420a63f54e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Metrics: classification, regression, clustering",
   "id": "68a8e4584f1a1f1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # basic classification metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score  # ROC/PR metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # confusion matrix/report\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # regression metrics\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score  # clustering metrics\n",
    "\n",
    "accuracy_score(y_test, y_test)  # proportion of correct predictions\n",
    "precision_score(y_test, y_test, average='binary')  # precision for positive class (for binary)\n",
    "recall_score(y_test, y_test, average='binary')  # recall for positive class (for binary)\n",
    "f1_score(y_test, y_test, average='binary')  # harmonic mean of precision and recall\n",
    "roc_auc_score(y_test, y_test)  # area under ROC curve (needs probabilities/scores for real use)\n",
    "roc_curve(y_test, y_test)  # ROC curve points (FPR, TPR, thresholds)\n",
    "precision_recall_curve(y_test, y_test)  # PR curve points (precision, recall, thresholds)\n",
    "average_precision_score(y_test, y_test)  # AP score summarizing PR curve\n",
    "confusion_matrix(y_test, y_test)  # confusion matrix (true vs predicted)\n",
    "classification_report(y_test, y_test)  # text report with precision/recall/F1 per class\n",
    "mean_squared_error(y_reg, y_reg)  # MSE for regression\n",
    "mean_absolute_error(y_reg, y_reg)  # MAE for regression\n",
    "r2_score(y_reg, y_reg)  # coefficient of determination R^2\n",
    "silhouette_score(X_iris, iris.target)  # clustering quality by cohesion/separation (needs labels)\n",
    "calinski_harabasz_score(X_iris, iris.target)  # variance ratio criterion for clusters\n",
    "davies_bouldin_score(X_iris, iris.target)  # average similarity between clusters\n"
   ],
   "id": "4b23198b2c7f5630"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model fitting, predicting, scoring (example pattern)",
   "id": "81725c7a33d21442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression  # example estimator\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)  # instantiate classifier\n",
    "clf.fit(X_train, y_train)  # fit model on training data\n",
    "clf.predict(X_test)  # predict class labels for test set\n",
    "clf.predict_proba(X_test)  # predict class probabilities for test set\n",
    "clf.score(X_test, y_test)  # compute mean accuracy on test data\n",
    "clf.get_params()  # retrieve hyperparameters as a dict\n",
    "clf.set_params(C=0.5)  # set/update hyperparameters\n"
   ],
   "id": "197702fc51a31922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspection and explainability helpers",
   "id": "54278e95bdc57a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.inspection import permutation_importance  # model-agnostic feature importance\n",
    "from sklearn.inspection import PartialDependenceDisplay  # partial dependence plots\n",
    "\n",
    "permutation_importance(clf, X_test, y_test, n_repeats=5, random_state=42)  # estimate feature importance via permutation\n",
    "# PartialDependenceDisplay.from_estimator(clf, X_test, features=[0, 1])  # plot PDP for features (uncomment in notebooks)\n"
   ],
   "id": "7afcc3a047e0ee06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text and feature extraction basics",
   "id": "9352d6bede022e04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # text vectorizers\n",
    "from sklearn.feature_extraction import DictVectorizer  # dict to feature matrix\n",
    "\n",
    "CountVectorizer(max_features=5000)  # convert text corpus to token count features\n",
    "TfidfVectorizer(max_features=5000)  # convert text to TF-IDF features\n",
    "DictVectorizer(sparse=True)  # convert list of mapping (dict) to feature matrix\n"
   ],
   "id": "999b6ea06b73f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline: typical end-to-end example (pattern)",
   "id": "58dbcc3dbc8b0b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import make_pipeline  # pipeline helper\n",
    "from sklearn.preprocessing import StandardScaler  # scaler\n",
    "from sklearn.svm import SVC  # classifier\n",
    "\n",
    "make_pipeline(StandardScaler(), SVC(probability=True, random_state=42))  # pipeline chaining preprocessing and model\n"
   ],
   "id": "14f66874bae6d82a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ColumnTransformer: heterogeneous data processing (pattern)",
   "id": "4618b6b99b7489da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.compose import ColumnTransformer  # column-wise transformer\n",
    "from sklearn.preprocessing import OneHotEncoder  # encoder\n",
    "import pandas as pd  # for column names example\n",
    "\n",
    "df_example = pd.DataFrame({  # small mixed-type frame\n",
    "    'num1': [0.5, 1.0, 1.5],\n",
    "    'cat1': ['a', 'b', 'a']\n",
    "})  # example data frame with numeric and categorical\n",
    "\n",
    "ColumnTransformer(transformers=[('num', StandardScaler(), ['num1']), ('cat', OneHotEncoder(handle_unknown='ignore'), ['cat1'])])  # apply per-column transforms\n"
   ],
   "id": "f64ddfa9291f5f9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering usage pattern (fit/predict-like)",
   "id": "855b109e8f5fe4a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans  # clustering estimator\n",
    "\n",
    "km = KMeans(n_clusters=3, n_init='auto', random_state=42)  # instantiate k-means\n",
    "km.fit(X_iris)  # learn cluster centers from data\n",
    "km.labels_  # get cluster labels assigned to training data\n",
    "km.cluster_centers_  # access learned cluster centers\n"
   ],
   "id": "909b62bb301b201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Persistence (save/load models) — commented to avoid I/O side effects",
   "id": "7e2d689859f25ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import joblib  # persistence utility for models\n",
    "# joblib.dump(clf, 'model.joblib')  # save trained model to disk\n",
    "# joblib.load('model.joblib')  # load model from disk\n"
   ],
   "id": "e03aecc85c9d2f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utilities and miscellaneous",
   "id": "3f892b6a16d7b0db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.utils import shuffle, resample  # data utilities\n",
    "from sklearn.utils.estimator_checks import check_estimator  # estimator API checks (advanced)\n",
    "\n",
    "shuffle(X_cls, y_cls, random_state=42)  # shuffle arrays in unison\n",
    "resample(X_cls, n_samples=100, replace=True, random_state=42)  # bootstrap resampling of data\n",
    "# check_estimator(LogisticRegression())  # run scikit-learn estimator checks (slow; uncomment for advanced use)\n"
   ],
   "id": "bac0215ba2f4276c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Time series and groups (splitting utilities examples)",
   "id": "3ddb5dd7a3f90280"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, LeaveOneGroupOut  # group-aware splitters\n",
    "\n",
    "GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=42)  # shuffle split while respecting group boundaries\n",
    "LeaveOneGroupOut()  # leave-one-group-out cross-validation splitter\n"
   ],
   "id": "786f00751a5b2729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "1. Some commands require providing an estimator; placeholders use `None` to illustrate signatures without executing.\n",
    "2. Plotting and disk I/O are commented to keep this script safe to run end-to-end.\n",
    "3. Replace dataset variables (`X`, `y`) with your data and uncomment relevant lines in a Jupyter environment for interactive exploration.\n",
    "4. Dataset fetchers that require network access (`fetch_*`, `fetch_openml`) are commented out to avoid downloads; uncomment when online.\n"
   ],
   "id": "dbdf695a46974357"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
