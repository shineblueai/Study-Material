{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scikit-Learn Pipelines — End-to-end ML lifecycle (one command per code cell)\n",
    "# This notebook-style script demonstrates how to use scikit-learn Pipelines across the data lifecycle:\n",
    "# data ingestion → validation → preprocessing/feature engineering → model training → tuning → evaluation → deployment → monitoring.\n",
    "# Each code cell contains one primary command with an inline comment explaining its purpose. Heavy I/O and network are commented.\n"
   ],
   "id": "57875dea53d99a25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation (commented) — run in your environment if needed\n",
    "# - Only install extras you actually need (faiss/mlflow/onnx/etc.)"
   ],
   "id": "16389e7557e66a6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install scikit-learn pandas numpy joblib  # core tools (commented)\n",
   "id": "afef72b331719547"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports and global config",
   "id": "75e43dc62584445c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import numpy as np  # numerical utilities used throughout\n",
   "id": "e74e11395c766914"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import pandas as pd  # tabular data handling used for ColumnTransformer with selectors\n",
   "id": "6c737492d5e6741d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn import set_config  # control estimator display and diagram rendering\n",
   "id": "f3004b23bd116597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "set_config(display='diagram')  # show nice diagrams for Pipeline/ColumnTransformer in rich notebook UIs\n",
   "id": "3f79fec030e75c69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample dataset — use a small built-in dataset for safe demos (no network)",
   "id": "9121e5aa6e02cc17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.datasets import load_iris  # classic multiclass classification dataset\n",
   "id": "6a549fa5671abf21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "iris = load_iris(as_frame=True)  # load Iris as a pandas-friendly Bunch (has .frame)\n",
   "id": "1e326311041b8fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = iris.frame  # pandas DataFrame with features + target for downstream ColumnTransformer demos\n",
   "id": "83539b7193a8873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X = df.drop(columns=['target'])  # features matrix as DataFrame (keeps dtypes for selectors)\n",
   "id": "21f8db0e9869b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y = df['target']  # target Series for classification\n",
   "id": "666205e1756ace9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data ingestion patterns (commented to avoid I/O/network)",
   "id": "42f30006bda8bc46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pd.read_csv('data/train.csv')  # read CSV from local path\n",
   "id": "25fd8242f841ed36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pd.read_parquet('data/train.parquet')  # read Parquet (requires pyarrow/fastparquet)\n",
   "id": "4dd4d76da2d56103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# import sqlalchemy; engine = sqlalchemy.create_engine('sqlite:///db.sqlite'); pd.read_sql('SELECT * FROM table', engine)  # SQL ingest (commented)\n",
   "id": "962f461611d4793c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from sklearn.datasets import fetch_openml; fetch_openml(name='titanic', version=1, as_frame=True)  # OpenML fetcher (may download)\n",
   "id": "7965a10b8a8ed9c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basic data validation and quick checks",
   "id": "81b9c5c85a977b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()  # concise summary: dtypes, non-nulls per column\n",
   "id": "d6f24f75ba6f0833"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe(include='all')  # quick stats overview (numeric and categorical)\n",
   "id": "4a5b058500577df3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.isna().sum()  # count missing values per column (sanity check before imputation)\n",
   "id": "e3a093acbad334ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train/validation/test splitting and CV splitters",
   "id": "cb35b44777229d3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.model_selection import train_test_split  # basic splitting utility\n",
   "id": "b286189fd9b14649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)  # stratified split for classification\n",
   "id": "4457c4f8b16cd891"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit  # common CV splitters\n",
   "id": "5f56578ebc93ca26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # use for CV maintaining label proportions\n",
   "id": "85444bd19a55ad66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "TimeSeriesSplit(n_splits=4)  # specialized splitter for ordered/time-indexed data\n",
   "id": "299130a4758b98a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing with ColumnTransformer — numeric + categorical branches",
   "id": "e9e1a5f26fc9df02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.compose import ColumnTransformer, make_column_selector  # column-wise transformer utilities\n",
   "id": "706cd271e3e46428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.impute import SimpleImputer  # impute missing values per column type\n",
   "id": "472af53ff10a77af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # scaling for numeric; OHE for categoricals\n",
   "id": "d29e50ac5c4a692c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "num_sel = make_column_selector(dtype_include=np.number)  # select numeric columns dynamically by dtype\n",
   "id": "72c05a1f39e5262d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cat_sel = make_column_selector(dtype_exclude=np.number)  # select non-numeric (categorical/object) columns\n",
   "id": "56653ed010c0641b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "numeric_pipeline = pd.Series([SimpleImputer(strategy='median'), StandardScaler()])  # placeholder to mirror steps idea (informational)\n",
   "id": "7033498856164514"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "categorical_pipeline = pd.Series([SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore')])  # placeholder steps (informational)\n",
   "id": "27b540c14ae3ed72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline := __import__('sklearn.pipeline').pipeline.Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())]), num_sel),\n",
    "        ('cat', Pipeline2 := __import__('sklearn.pipeline').pipeline.Pipeline([('impute', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat_sel),\n",
    "    ]\n",
    ")  # build ColumnTransformer with nested Pipelines per column type\n"
   ],
   "id": "c943e57678b44684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature engineering primitives (examples)",
   "id": "9e718f33a590d9ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, FunctionTransformer  # engineered features\n",
   "id": "48d452fe87758182"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "PolynomialFeatures(degree=2, include_bias=False)  # generate polynomial/interaction terms for numeric features\n",
   "id": "2802e11acb0ed3a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "KBinsDiscretizer(n_bins=5, encode='onehot', strategy='quantile')  # bucketize continuous features into bins\n",
   "id": "759e2fd3e84c2b42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "FunctionTransformer(np.log1p, feature_names_out='one-to-one')  # wrap numpy transforms to use in pipelines\n",
   "id": "9aad902c081ae5e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Target transformation wrapper (for regression pipelines)",
   "id": "1a03f6b8ba0e60ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.compose import TransformedTargetRegressor  # apply transform to y during fit/predict\n",
   "id": "454fe89c6ea0bb2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# End-to-end Pipeline: preprocessing + model",
   "id": "89060b9ed5f5d8d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.pipeline import Pipeline, make_pipeline  # pipeline composition helpers\n",
   "id": "120122cff9ea2c24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.linear_model import LogisticRegression  # classifier for iris demo\n",
   "id": "16699e21c06e5d97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf_pipe = Pipeline(steps=[('prep', preprocess), ('model', LogisticRegression(max_iter=1000, random_state=42))])  # define full pipeline\n",
   "id": "a3a538f9a1d92f4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf_pipe.fit(X_train, y_train)  # fit preprocessing + model in one step on training data\n",
   "id": "7fe1a0c09ddec4e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred = clf_pipe.predict(X_test)  # run inference through the whole pipeline on test data\n",
   "id": "e1bdede9dd784d6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf_pipe.score(X_test, y_test)  # compute pipeline accuracy on held-out test set\n",
   "id": "a9eef45b9cf3f0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross-validation with pipelines",
   "id": "4b30145e1b8011f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.model_selection import cross_validate  # evaluate with CV across folds\n",
   "id": "71f425f0513f28a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cross_validate(clf_pipe, X, y, cv=5, scoring=['accuracy', 'f1_macro'], return_train_score=False)  # multi-metric CV on the full pipeline\n",
   "id": "90c0c01895f074c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter search — grid/random search over pipeline parameters",
   "id": "ee23391debce4d4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  # model selection tools\n",
   "id": "ca3114fdc73baff5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "param_grid = {'model__C': [0.1, 1.0, 10.0], 'model__penalty': ['l2'], 'model__solver': ['lbfgs']}  # reference step params with step__param\n",
   "id": "8124ee0008a66ad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "grid = GridSearchCV(clf_pipe, param_grid=param_grid, cv=5, n_jobs=None)  # CV grid search across hyperparameters\n",
   "id": "a347b7ee103cea20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# grid.fit(X, y)  # run search (commented to keep notebook fast); uncomment to execute and inspect grid.best_params_\n",
   "id": "14700fc881cd5219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "rand = RandomizedSearchCV(clf_pipe, param_distributions={'model__C': np.logspace(-3, 1, 20)}, n_iter=8, cv=5, random_state=42)  # randomized search stub\n",
   "id": "f829cb34e75f3377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Access transformed feature names (after fitting the preprocess step)",
   "id": "240956e01020d2a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf_pipe.named_steps['prep'].get_feature_names_out()  # list output feature names from ColumnTransformer (after fit)\n",
   "id": "1308c40d288b89ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calibration and thresholding (classification)",
   "id": "d8570a7caeeefb2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.calibration import CalibratedClassifierCV  # wrap classifier to calibrate predicted probabilities\n",
   "id": "6214b9aaf3063e18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "calibrated_pipe = Pipeline(steps=[('prep', preprocess), ('cal', CalibratedClassifierCV(LogisticRegression(max_iter=1000, random_state=42), cv=3, method='isotonic'))])  # pipeline with probability calibration\n",
   "id": "f55ed2f82d9ae859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "calibrated_pipe.fit(X_train, y_train)  # fit calibrated pipeline on training data\n",
   "id": "2491ab6ec845e428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "proba = calibrated_pipe.predict_proba(X_test)[:, 1 if len(np.unique(y))==2 else 0]  # get calibrated probabilities (choose a class index)\n",
   "id": "aaa9a98c4732992a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.metrics import precision_recall_curve  # curve for threshold selection\n",
   "id": "79a66afec154ec3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "precision_recall_curve((y_test==np.unique(y)[0]).astype(int) if len(np.unique(y))>2 else y_test, proba)  # PR curve tuples for threshold tuning (demo)\n",
   "id": "307c337af8cadc6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation metrics with pipelines",
   "id": "4c2ab0138e76abe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix  # common metrics\n",
   "id": "b08d2091d51d6dee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "accuracy_score(y_test, y_pred)  # compute accuracy using pipeline predictions\n",
   "id": "4771f3d16051c88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "f1_score(y_test, y_pred, average='macro')  # compute macro-averaged F1 score on multiclass iris\n",
   "id": "4482a8dfadfa1a49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "classification_report(y_test, y_pred)  # text report of precision/recall/F1 per class\n",
   "id": "53da86b634e8d09e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "confusion_matrix(y_test, y_pred)  # confusion matrix for error analysis\n",
   "id": "46cd76314533b884"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regression example: Pipeline + TransformedTargetRegressor (log-transform target)",
   "id": "d0bbd7923fef09f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.datasets import make_regression  # synthetic regression data\n",
   "id": "b6b55cfbde40f627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Xr, yr = make_regression(n_samples=200, n_features=6, noise=10.0, random_state=42)  # small synthetic regression dataset\n",
   "id": "d017bb92f21959e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.linear_model import Ridge  # regularized linear regressor\n",
   "id": "eab500a66adad957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "reg_prep = ColumnTransformer([('num', __import__('sklearn.pipeline').pipeline.Pipeline([('impute', SimpleImputer()), ('scale', StandardScaler())]), list(range(Xr.shape[1])))])  # numeric-only preprocess for regression\n",
   "id": "d5b09e42e523d529"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "reg_pipe = Pipeline([('prep', reg_prep), ('model', TransformedTargetRegressor(regressor=Ridge(alpha=1.0, random_state=42), func=np.log1p, inverse_func=np.expm1))])  # wrap target transform\n",
   "id": "dd4547b888bdc94d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "reg_pipe.fit(Xr, yr)  # fit regression pipeline end-to-end\n",
   "id": "5c6fd109fa8a62c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "reg_pipe.score(Xr, yr)  # R^2 score on synthetic data (demo)\n",
   "id": "48c81ff14ed220ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deployment: persist and load trained pipelines (commented to avoid I/O)",
   "id": "a98e5144f338fc6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# import joblib; joblib.dump(clf_pipe, 'iris_pipeline.joblib')  # save fitted pipeline to disk\n",
   "id": "bb237fa76657ec8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# import joblib; loaded = joblib.load('iris_pipeline.joblib'); loaded.predict(X_test)  # load and run inference (commented)\n",
   "id": "38e4f919b4426de3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference pattern: predict on a new sample row",
   "id": "4873baf6a7a55a3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sample_row = X_test.iloc[[0]]  # take one row as a tiny batch (keeps DataFrame structure for preprocess selectors)\n",
   "id": "97e4975f6522def7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf_pipe.predict(sample_row)  # run prediction through full pipeline on a single sample\n",
   "id": "eee2d187c4163702"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Monitoring: compute rolling/periodic metrics and calibration; simple drift proxy",
   "id": "479d04d2f2e18eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.metrics import brier_score_loss  # calibration-sensitive loss for probabilities\n",
   "id": "bc46255c5461d274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.model_selection import cross_val_predict  # out-of-fold predictions for unbiased monitoring estimates\n",
   "id": "9849bcd4c6e77946"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "oof_proba = cross_val_predict(clf_pipe, X, y, cv=5, method='predict_proba')  # oof probability predictions using full pipeline\n",
   "id": "e49f0afae6fb40a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "brier_score_loss((y==np.unique(y)[0]).astype(int) if len(np.unique(y))>2 else y, oof_proba[:, 1 if len(np.unique(y))==2 else 0])  # baseline Brier score as monitoring metric\n",
   "id": "77f422490414d480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.DataFrame(X_train).mean(numeric_only=True)  # reference training means (toy drift baseline over numeric features)\n",
   "id": "9f33835213e1dbfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.DataFrame(X_test).mean(numeric_only=True)  # compare new batch means vs. training (manual drift check idea)\n",
   "id": "cd1bb23b1b9bf99b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Production tips and optional integrations (commented)\n",
    "# - Class imbalance: use imbalanced-learn `Pipeline` with `SMOTE` inside CV only (to avoid leakage).\n",
    "# - Model registry/tracking: MLflow `mlflow.sklearn.log_model`, experiments, metrics.\n",
    "# - Export for other runtimes: `skl2onnx` to ONNX, or `joblib` persistence for Python services.\n",
    "# - Batch/online inference: keep the same preprocessing inside the pipeline to avoid train/serve skew."
   ],
   "id": "7fea161f0a41d878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # imbalanced-learn example (commented):\n",
    "# # !pip install imbalanced-learn\n",
    "# # from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# # from imblearn.over_sampling import SMOTE\n",
    "# # imb_pipe = ImbPipeline([('prep', preprocess), ('smote', SMOTE(random_state=42)), ('model', LogisticRegression(max_iter=1000))])\n"
   ],
   "id": "a6354435ff3b0a3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
