{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlit Resume Parser & Matcher\n",
    "Upload a resume (PDF/DOCX/TXT), extract text, parse structured fields, and optionally enhance parsing via LLMs. Match against a job description.\n"
   ],
   "id": "c1ac34cad9fea9c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Installation (commented)",
   "id": "8a62b9d7784aff1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install streamlit pypdf docx2txt sentence-transformers openai anthropic google-generativeai groq huggingface_hub\n",
   "id": "de9ed3274c307220"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "ac077b516c4b9e97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import streamlit as st\n"
   ],
   "id": "12b83d0c7acab327"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# File extraction helpers",
   "id": "ed611cb49d4eb122"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_text(file) -> str:\n",
    "    name = file.name.lower()\n",
    "    data = file.read()\n",
    "    if name.endswith('.pdf'):\n",
    "        from pypdf import PdfReader\n",
    "        reader = PdfReader(io.BytesIO(data))\n",
    "        return \"\\n\".join([(p.extract_text() or '') for p in reader.pages])\n",
    "    if name.endswith('.docx'):\n",
    "        import docx2txt\n",
    "        bio = io.BytesIO(data)\n",
    "        # docx2txt expects a file path; workaround: save to temp if needed\n",
    "        # For simplicity, try in-memory via temporary write (commented for sandbox)\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(suffix='.docx', delete=True) as tmp:\n",
    "            tmp.write(bio.getbuffer())\n",
    "            tmp.flush()\n",
    "            return docx2txt.process(tmp.name) or ''\n",
    "    # fallback for txt\n",
    "    return data.decode('utf-8', errors='ignore')\n"
   ],
   "id": "a7341238c08a2d43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple regex-based parsing (baseline)",
   "id": "be94ca568a2edfac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_baseline(text: str) -> dict:\n",
    "    email = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)\n",
    "    phone = re.search(r\"(\\+\\d{1,3}[- ]?)?\\d{10}\", text)\n",
    "    linkedin = re.search(r\"https?://(www\\.)?linkedin\\.com/[A-Za-z0-9_\\-/]+\", text)\n",
    "    github = re.search(r\"https?://(www\\.)?github\\.com/[A-Za-z0-9_\\-/]+\", text)\n",
    "    name_line = text.strip().splitlines()[0] if text.strip().splitlines() else ''\n",
    "    return {\n",
    "        'name': name_line.strip(),\n",
    "        'email': email.group(0) if email else '',\n",
    "        'phone': phone.group(0) if phone else '',\n",
    "        'linkedin': linkedin.group(0) if linkedin else '',\n",
    "        'github': github.group(0) if github else '',\n",
    "        'summary': '',\n",
    "        'skills': [],\n",
    "        'experience': [],\n",
    "        'education': [],\n",
    "        'raw_text': text,\n",
    "    }\n"
   ],
   "id": "463f5ff1ffa894b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optional: LLM-enhanced parsing",
   "id": "941d85b3f2d8a595"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def llm_parse(text: str, provider: str, model: str, temperature: float = 0.0, max_tokens: int = 512) -> dict:\n",
    "    schema = {\n",
    "        'name': 'string', 'email': 'string', 'phone': 'string', 'location': 'string',\n",
    "        'summary': 'string', 'skills': ['string'], 'experience': [{'company':'string','title':'string','start':'string','end':'string','desc':'string'}],\n",
    "        'education': [{'school':'string','degree':'string','year':'string'}], 'links': ['string']\n",
    "    }\n",
    "    instruction = (\n",
    "        \"Extract structured resume information as strict JSON matching this schema keys: \"\n",
    "        f\"{list(schema.keys())}. Use empty strings/lists when unknown.\"\n",
    "    )\n",
    "    if provider == 'OpenAI':\n",
    "        from openai import OpenAI\n",
    "        if (k:=st.secrets.get('OPENAI_API_KEY', None) if hasattr(st,'secrets') else None) or os.environ.get('OPENAI_API_KEY'):\n",
    "            os.environ['OPENAI_API_KEY'] = k or os.environ.get('OPENAI_API_KEY','')\n",
    "        client = OpenAI()\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            response_format={'type':'json_object'},\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\":\"system\",\"content\":instruction},{\"role\":\"user\",\"content\":text}],\n",
    "        )\n",
    "        return json.loads(resp.choices[0].message.content)\n",
    "    if provider == 'Gemini':\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=st.secrets.get('GEMINI_API_KEY', os.environ.get('GEMINI_API_KEY')))\n",
    "        g = genai.GenerativeModel(model)\n",
    "        out = g.generate_content(f\"{instruction}\\n\\n{text}\")\n",
    "        try:\n",
    "            return json.loads(out.text)\n",
    "        except Exception:\n",
    "            return {'error':'Failed to parse JSON from Gemini', 'raw': getattr(out,'text', str(out))}\n",
    "    if provider == 'Anthropic':\n",
    "        import anthropic\n",
    "        a = anthropic.Anthropic()\n",
    "        out = a.messages.create(model=model, max_tokens=max_tokens, temperature=temperature,\n",
    "                                messages=[{\"role\":\"user\",\"content\":instruction + \"\\n\\n\" + text}])\n",
    "        try:\n",
    "            return json.loads(out.content[0].text)\n",
    "        except Exception:\n",
    "            return {'error':'Failed to parse JSON from Claude', 'raw': out.content[0].text}\n",
    "    if provider == 'Groq':\n",
    "        from groq import Groq\n",
    "        gq = Groq(api_key=st.secrets.get('GROQ_API_KEY', os.environ.get('GROQ_API_KEY')))\n",
    "        out = gq.chat.completions.create(model=model, temperature=temperature, max_tokens=max_tokens,\n",
    "            messages=[{\"role\":\"system\",\"content\":\"Return ONLY valid JSON.\"},{\"role\":\"user\",\"content\":instruction + \"\\n\\n\" + text}],\n",
    "        )\n",
    "        try:\n",
    "            return json.loads(out.choices[0].message.content)\n",
    "        except Exception:\n",
    "            return {'error':'Failed to parse JSON from Groq', 'raw': out.choices[0].message.content}\n",
    "    if provider == 'HuggingFace':\n",
    "        from huggingface_hub import InferenceClient\n",
    "        hf = InferenceClient(token=st.secrets.get('HUGGINGFACEHUB_API_TOKEN', os.environ.get('HUGGINGFACEHUB_API_TOKEN')))\n",
    "        try:\n",
    "            resp = hf.chat_completion(model=model, messages=[{\"role\":\"user\",\"content\":instruction + \"\\n\\n\" + text}], max_tokens=max_tokens)\n",
    "            return json.loads(resp.choices[0].message['content'] if hasattr(resp.choices[0],'message') else resp.choices[0]['message']['content'])\n",
    "        except Exception as e:\n",
    "            return {'error': f'HF error {e}'}\n",
    "    raise ValueError('Unsupported provider')\n"
   ],
   "id": "6b8ede0e1352f0fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Matching score (JD â†” resume) using sentence-transformers (optional)",
   "id": "c7b7b2eef88ea52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def similarity_score(resume_text: str, jd_text: str) -> float:\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        e1 = model.encode([resume_text], normalize_embeddings=True)\n",
    "        e2 = model.encode([jd_text], normalize_embeddings=True)\n",
    "        return float(util.cos_sim(e1, e2)[0][0])\n",
    "    except Exception:\n",
    "        # Fallback: simple token overlap ratio\n",
    "        rs = set(re.findall(r\"\\w+\", resume_text.lower()))\n",
    "        js = set(re.findall(r\"\\w+\", jd_text.lower()))\n",
    "        inter = len(rs & js)\n",
    "        uni = len(rs | js) or 1\n",
    "        return inter / uni\n"
   ],
   "id": "bf5c56274ea405dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UI",
   "id": "f7b312282baa01f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st.set_page_config(page_title=\"Resume Parser\", page_icon=\"ðŸ“„\")\n",
    "st.title(\"ðŸ“„ Resume Parser & Matcher\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header('LLM (optional)')\n",
    "    prov = st.selectbox('Provider', ['None','OpenAI','Gemini','Anthropic','Groq','HuggingFace'], index=0)\n",
    "    if prov != 'None':\n",
    "        if prov=='OpenAI':\n",
    "            model = st.text_input('Model', 'gpt-4o-mini')\n",
    "        elif prov=='Gemini':\n",
    "            model = st.text_input('Model', 'gemini-1.5-flash')\n",
    "        elif prov=='Anthropic':\n",
    "            model = st.text_input('Model', 'claude-3-5-sonnet-20241022')\n",
    "        elif prov=='Groq':\n",
    "            model = st.text_input('Model', 'llama-3.1-8b-instant')\n",
    "        else:\n",
    "            model = st.text_input('Model', 'Qwen/Qwen2.5-1.5B-Instruct')\n",
    "    else:\n",
    "        model = ''\n",
    "\n",
    "left, right = st.columns(2)\n",
    "with left:\n",
    "    resume_file = st.file_uploader('Upload resume (PDF/DOCX/TXT)', type=['pdf','docx','txt'])\n",
    "with right:\n",
    "    jd_text = st.text_area('Paste job description (optional)', height=200)\n"
   ],
   "id": "6309a45c4695e12c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run parsing",
   "id": "7324a6964cbc8271"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if st.button('Parse resume', type='primary') and resume_file is not None:\n",
    "    text = extract_text(resume_file)\n",
    "    parsed = parse_baseline(text)\n",
    "    if prov != 'None':\n",
    "        try:\n",
    "            parsed_llm = llm_parse(text, prov, model)\n",
    "            parsed.update({k: parsed_llm.get(k, parsed.get(k)) for k in parsed_llm if k in parsed})\n",
    "            parsed['llm'] = parsed_llm\n",
    "        except Exception as e:\n",
    "            st.warning(f\"LLM parse failed: {e}\")\n",
    "    st.subheader('Parsed JSON')\n",
    "    st.json(parsed)\n",
    "    if jd_text.strip():\n",
    "        score = similarity_score(text, jd_text)\n",
    "        st.metric('Match score (0-1 ~ cosine)', f\"{score:.3f}\")\n",
    "    st.download_button('Download JSON', data=json.dumps(parsed, ensure_ascii=False, indent=2), file_name='resume.json', mime='application/json')\n"
   ],
   "id": "5ebb05bb9ecb8251"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "# - LLM parsing may incur costs; ensure keys are set in secrets or env.\n",
    "# - Improve baseline parsing by adding section detection and skill dictionaries."
   ],
   "id": "457b7965cda87a5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "864bcbd67e01cf53"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
