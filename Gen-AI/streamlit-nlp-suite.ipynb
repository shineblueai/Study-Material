{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlit NLP Suite â€” Sentiment, NER, Summarization, Keywords\n",
    "Run classic NLP tasks via Transformers pipelines or LLM providers (OpenAI, Gemini, Anthropic, Groq, HF Inference) from a simple Streamlit UI.\n"
   ],
   "id": "a69372a0b51062c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Installation (commented)",
   "id": "4cb81027eb028945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# !pip install streamlit transformers sentence-transformers spacy openai anthropic google-generativeai groq huggingface_hub\n",
    "# !python -m spacy download en_core_web_sm  # optional for local NER\n"
   ],
   "id": "e9205005d44cb812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "77e040f20e766c67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import streamlit as st\n"
   ],
   "id": "b99ff2fd3f68c356"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper: provider calls (LLM-based)",
   "id": "c7f063c250399695"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def llm_complete(task_prompt: str, provider: str, model: str, temperature: float, max_tokens: int) -> str:\n",
    "    if provider == 'OpenAI':\n",
    "        from openai import OpenAI\n",
    "        if (k:=st.secrets.get('OPENAI_API_KEY', None) if hasattr(st,'secrets') else None) or os.environ.get('OPENAI_API_KEY'):\n",
    "            os.environ['OPENAI_API_KEY'] = k or os.environ.get('OPENAI_API_KEY','')\n",
    "        client = OpenAI()\n",
    "        out = client.chat.completions.create(model=model, temperature=temperature, max_tokens=max_tokens,\n",
    "                                             messages=[{\"role\":\"system\",\"content\":\"You are an NLP assistant.\"}, {\"role\":\"user\",\"content\":task_prompt}])\n",
    "        return out.choices[0].message.content\n",
    "    if provider == 'Gemini':\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=st.secrets.get('GEMINI_API_KEY', os.environ.get('GEMINI_API_KEY')))\n",
    "        g = genai.GenerativeModel(model)\n",
    "        res = g.generate_content(task_prompt)\n",
    "        return getattr(res,'text',str(res))\n",
    "    if provider == 'Anthropic':\n",
    "        import anthropic\n",
    "        a = anthropic.Anthropic()\n",
    "        res = a.messages.create(model=model, max_tokens=max_tokens, temperature=temperature, messages=[{\"role\":\"user\",\"content\":task_prompt}])\n",
    "        return res.content[0].text\n",
    "    if provider == 'Groq':\n",
    "        from groq import Groq\n",
    "        gq = Groq(api_key=st.secrets.get('GROQ_API_KEY', os.environ.get('GROQ_API_KEY')))\n",
    "        res = gq.chat.completions.create(model=model, temperature=temperature, max_tokens=max_tokens,\n",
    "                                         messages=[{\"role\":\"system\",\"content\":\"You are an NLP assistant.\"}, {\"role\":\"user\",\"content\":task_prompt}])\n",
    "        return res.choices[0].message.content\n",
    "    if provider == 'HuggingFace':\n",
    "        from huggingface_hub import InferenceClient\n",
    "        hf = InferenceClient(token=st.secrets.get('HUGGINGFACEHUB_API_TOKEN', os.environ.get('HUGGINGFACEHUB_API_TOKEN')))\n",
    "        try:\n",
    "            res = hf.chat_completion(model=model, messages=[{\"role\":\"user\",\"content\":task_prompt}], max_tokens=max_tokens)\n",
    "            return res.choices[0].message['content'] if hasattr(res.choices[0],'message') else res.choices[0]['message']['content']\n",
    "        except Exception:\n",
    "            return hf.text_generation(model=model, inputs=task_prompt, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    raise ValueError('Unsupported provider')\n"
   ],
   "id": "b4d236ae2379814d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper: transformers pipelines (locally)",
   "id": "1dca166deb76ad9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_pipeline(task: str, model: str | None = None):\n",
    "    from transformers import pipeline\n",
    "    return pipeline(task) if not model else pipeline(task, model=model)\n"
   ],
   "id": "8479b113cd9c23f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UI",
   "id": "7a7a7651ca99f54d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st.set_page_config(page_title=\"NLP Suite\", page_icon=\"ðŸ§ \")\n",
    "st.title(\"ðŸ§  NLP Suite â€” Transformers & LLMs\")\n",
    "\n",
    "with st.sidebar:\n",
    "    backend = st.selectbox('Backend', ['Transformers (local)', 'LLM Provider'], index=0)\n",
    "    if backend == 'LLM Provider':\n",
    "        provider = st.selectbox('Provider', ['OpenAI','Gemini','Anthropic','Groq','HuggingFace'], index=0)\n",
    "        if provider=='OpenAI':\n",
    "            model = st.text_input('Model', 'gpt-4o-mini')\n",
    "        elif provider=='Gemini':\n",
    "            model = st.text_input('Model', 'gemini-1.5-flash')\n",
    "        elif provider=='Anthropic':\n",
    "            model = st.text_input('Model', 'claude-3-5-sonnet-20241022')\n",
    "        elif provider=='Groq':\n",
    "            model = st.text_input('Model', 'llama-3.1-8b-instant')\n",
    "        else:\n",
    "            model = st.text_input('Model', 'Qwen/Qwen2.5-1.5B-Instruct')\n",
    "        temperature = st.slider('temperature', 0.0, 1.5, 0.2, 0.1)\n",
    "        max_tokens = st.slider('max tokens', 64, 2048, 256, 32)\n",
    "    else:\n",
    "        provider = model = None\n",
    "        temperature = 0.0\n",
    "        max_tokens = 256\n",
    "\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"Sentiment\", \"NER\", \"Summarization\", \"Keywords\"])\n",
    "\n",
    "with tab1:\n",
    "    st.subheader('Sentiment Analysis')\n",
    "    text = st.text_area('Input text', height=120)\n",
    "    if st.button('Analyze sentiment', key='sentiment_btn') and text.strip():\n",
    "        if backend.startswith('Transformers'):\n",
    "            clf = get_pipeline('sentiment-analysis')\n",
    "            res = clf(text)\n",
    "            st.json(res)\n",
    "        else:\n",
    "            prompt = f\"Classify sentiment as POSITIVE or NEGATIVE with confidence.\\nText: {text}\"\n",
    "            out = llm_complete(prompt, provider, model, temperature, max_tokens)\n",
    "            st.write(out)\n",
    "\n",
    "with tab2:\n",
    "    st.subheader('Named Entity Recognition')\n",
    "    text2 = st.text_area('Input text for NER', height=120)\n",
    "    if st.button('Extract entities', key='ner_btn') and text2.strip():\n",
    "        if backend.startswith('Transformers'):\n",
    "            ner = get_pipeline('ner')\n",
    "            res = ner(text2)\n",
    "            st.json(res)\n",
    "        else:\n",
    "            prompt = \"\"\"Extract named entities from the text and return JSON with keys: person, org, location, date, other.\\nText: \"\"\" + text2\n",
    "            out = llm_complete(prompt, provider, model, temperature, max_tokens)\n",
    "            st.write(out)\n",
    "\n",
    "with tab3:\n",
    "    st.subheader('Summarization')\n",
    "    text3 = st.text_area('Input text to summarize', height=160)\n",
    "    if st.button('Summarize', key='sum_btn') and text3.strip():\n",
    "        if backend.startswith('Transformers'):\n",
    "            summ = get_pipeline('summarization')\n",
    "            res = summ(text3)\n",
    "            st.json(res)\n",
    "        else:\n",
    "            prompt = f\"Summarize the following text in 3-5 bullet points.\\n\\n{text3}\"\n",
    "            out = llm_complete(prompt, provider, model, temperature, max_tokens)\n",
    "            st.write(out)\n",
    "\n",
    "with tab4:\n",
    "    st.subheader('Keyword Extraction')\n",
    "    text4 = st.text_area('Input text for keywords', height=140)\n",
    "    if st.button('Extract keywords', key='kw_btn') and text4.strip():\n",
    "        if backend.startswith('Transformers'):\n",
    "            # Simple local heuristic via TextRank or KeyBERT could be added; here, use a small prompt to a local model if available.\n",
    "            try:\n",
    "                from transformers import pipeline\n",
    "                gen = pipeline('text-generation', model='gpt2')\n",
    "                res = gen(f\"Extract 5-10 keywords: {text4}\", max_new_tokens=30)\n",
    "                st.json(res)\n",
    "            except Exception as e:\n",
    "                st.warning(f\"Local keyword extraction not configured: {e}\")\n",
    "        else:\n",
    "            prompt = f\"Extract 5-10 keywords (comma-separated) capturing main topics from the text.\\nText: {text4}\"\n",
    "            out = llm_complete(prompt, provider, model, temperature, max_tokens)\n",
    "            st.write(out)\n"
   ],
   "id": "4f7982d0ab7730af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "# - Transformers pipelines may download models on first run.\n",
    "# - For production, prefer deterministic prompts and JSON outputs with schema validation."
   ],
   "id": "9eed86c3f61ac4d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48fd28b1555427c2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
