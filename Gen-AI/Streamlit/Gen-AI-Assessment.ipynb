{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gen-AI — Assessment\n",
    "\n",
    "This assessment aligns with notebooks in `Gen-AI/Streamlit/` (RAG, embeddings, chatbots, translator, resume-parser, multimodal, etc.). Focus: concepts of embeddings, similarity search, chunking, simple retrieval, prompt design basics, and evaluation.\n",
    "\n",
    "Total questions: 25 (10 Theory, 8 Fill-in-the-Blanks, 7 Coding). Difficulty mix: 40% easy, 40% medium, 20% hard.\n"
   ],
   "id": "a8e70beb87cb640f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instructions\n",
    "- Answer all questions.\n",
    "- Coding tasks avoid heavy dependencies; we use NumPy and pure Python to simulate retrieval.\n",
    "- Solutions at the bottom.\n"
   ],
   "id": "eed4c7dce8c2e03a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- All notebooks under `Gen-AI/Streamlit/` (e.g., `streamlit-rag.ipynb`, `streamlit-embeddings.ipynb`, `streamlit-chatbot.ipynb`, `streamlit-multimodal.ipynb`)\n"
   ],
   "id": "93a3bbaebb737fa2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part A — Theory (10)\n",
    "1. Define an embedding. Why are embeddings useful for retrieval?\n",
    "2. MCQ: Which similarity is commonly used with normalized embeddings? (a) L1 distance (b) cosine similarity (c) Jaccard distance (d) Hamming distance\n",
    "3. Explain chunking in RAG and its tradeoff between chunk size and recall/precision.\n",
    "4. What is a vector store? Mention two examples of indexes or libraries.\n",
    "5. MCQ: Which mitigates prompt injection best? (a) ignore it (b) restrict tool scope and sanitize inputs (c) longer prompts (d) higher temperature\n",
    "6. Describe the retrieval step in RAG and how re-ranking can improve results.\n",
    "7. What is the difference between system, user, and assistant messages in chat prompting?\n",
    "8. Explain context window limits and strategies to fit more context.\n",
    "9. MCQ: Which helps reduce hallucinations? (a) increase temperature (b) decrease top_p (c) ground answers with retrieved context and cite sources (d) no change\n",
    "10. What are evaluation strategies for RAG beyond accuracy? Name two.\n"
   ],
   "id": "4df24722ca6d3295"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part B — Fill in the Blanks (8)\n",
    "1. Cosine similarity ranges from ______ to ______ for real-valued vectors.\n",
    "2. Normalizing vectors to unit length is called L2 __________.\n",
    "3. Splitting documents into smaller parts is called __________.\n",
    "4. In RAG, the generator is conditioned on retrieved __________.\n",
    "5. A hybrid retriever combines sparse and __________ retrieval.\n",
    "6. To prevent long-context costs, we can do __________ re-ranking.\n",
    "7. Prompt templates often contain named __________ that are filled at runtime.\n",
    "8. Storing embeddings and metadata enables filtered __________.\n"
   ],
   "id": "1de4610ef9bbc9f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part C — Coding Tasks (7)\n",
    "Implement the functions with NumPy and Python. Run asserts.\n",
    "\n",
    "Tasks:\n",
    "1. `l2_normalize(X)` — row-wise L2 normalize a 2D array.\n",
    "2. `cosine_sim_matrix(A, B)` — return cosine similarity matrix [len(A), len(B)].\n",
    "3. `simple_chunk(text, max_len, overlap)` — split text into chunks of up to `max_len` with token overlap (approximate by characters).\n",
    "4. `top_k_sim(query_vec, doc_vecs, k)` — return indices of top-k most similar documents by cosine.\n",
    "5. `retrieve(query, docs, embed_fn, k)` — embed query and docs using `embed_fn(text)->vec`; return top-k docs by cosine.\n",
    "6. `rerank(query, candidates, score_fn)` — given initial candidate docs, return them sorted by `score_fn(query, doc)` descending.\n",
    "7. `fill_template(tpl, values)` — replace `{name}` placeholders in `tpl` with `values[name]`; leave unknown placeholders intact.\n"
   ],
   "id": "307839c1cd4024ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def l2_normalize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, float)\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms = np.where(norms==0, 1.0, norms)\n",
    "    return X / norms\n",
    "\n",
    "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    A = l2_normalize(A)\n",
    "    B = l2_normalize(B)\n",
    "    return A @ B.T\n",
    "\n",
    "def simple_chunk(text: str, max_len: int, overlap: int) -> list:\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    step = max_len - overlap\n",
    "    step = max(1, step)\n",
    "    while i < n:\n",
    "        chunks.append(text[i:i+max_len])\n",
    "        i += step\n",
    "    return chunks\n",
    "\n",
    "def top_k_sim(query_vec: np.ndarray, doc_vecs: np.ndarray, k: int) -> list:\n",
    "    sims = cosine_sim_matrix(query_vec[None, :], doc_vecs)[0]\n",
    "    idx = np.argpartition(-sims, k-1)[:k]\n",
    "    return list(idx[np.argsort(-sims[idx])])\n",
    "\n",
    "def retrieve(query: str, docs: list, embed_fn, k: int = 3) -> list:\n",
    "    qv = embed_fn(query)\n",
    "    dv = np.vstack([embed_fn(d) for d in docs])\n",
    "    top = top_k_sim(qv, dv, min(k, len(docs)))\n",
    "    return [docs[i] for i in top]\n",
    "\n",
    "def rerank(query: str, candidates: list, score_fn) -> list:\n",
    "    scored = [(doc, score_fn(query, doc)) for doc in candidates]\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [d for d,_ in scored]\n",
    "\n",
    "def fill_template(tpl: str, values: dict) -> str:\n",
    "    def repl(m):\n",
    "        key = m.group(1)\n",
    "        return str(values.get(key, m.group(0)))\n",
    "    return re.sub(r\"\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}\", repl, tpl)\n"
   ],
   "id": "75ecf4f1a4fcd2e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Asserts\n",
    "X = np.array([[3,4],[0,0]], float)\n",
    "LX = l2_normalize(X)\n",
    "assert np.allclose(np.linalg.norm(LX[0]), 1.0)\n",
    "assert np.allclose(LX[1], [0,0])\n",
    "\n",
    "A = np.array([[1,0],[0,1]], float)\n",
    "B = np.array([[1,0],[1,1]], float)\n",
    "S = cosine_sim_matrix(A,B)\n",
    "assert S.shape == (2,2)\n",
    "assert S[0,0] > S[0,1]\n",
    "\n",
    "txt = \"abcdefghij\"\n",
    "chs = simple_chunk(txt, 4, 2)\n",
    "assert chs[0] == 'abcd' and chs[1].startswith('cd')\n",
    "\n",
    "q = np.array([1.0,0.0])\n",
    "D = np.array([[1.0,0.0],[0.0,1.0],[0.7,0.7]])\n",
    "top = top_k_sim(q, D, 2)\n",
    "assert top[0] == 0\n",
    "\n",
    "def toy_embed(s: str):\n",
    "    # simple 2D: [count of vowels, count of consonants]\n",
    "    v = sum(c.lower() in 'aeiou' for c in s)\n",
    "    c = sum(c.isalpha() and c.lower() not in 'aeiou' for c in s)\n",
    "    return np.array([v, c], float)\n",
    "\n",
    "docs = ['alpha', 'beta', 'queue']\n",
    "ret = retrieve('aei', docs, toy_embed, 2)\n",
    "assert len(ret) == 2\n",
    "\n",
    "rer = rerank('z', docs, lambda q,d: -abs(len(d)-3))\n",
    "assert isinstance(rer, list)\n",
    "\n",
    "out = fill_template('Hello {name}, today is {day}.', {'name': 'Sam'})\n",
    "assert out.startswith('Hello Sam') and '{day}' in out\n",
    "\n",
    "print('Gen-AI asserts passed ✅')\n"
   ],
   "id": "4b549c6f6b375955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Solutions\n",
    "\n",
    "### Theory (sample)\n",
    "1. Embedding: numeric vector capturing semantics; enables similarity search.\n",
    "2. (b) cosine similarity\n",
    "3. Chunking splits documents; larger chunks improve precision but can reduce recall and fit; smaller chunks increase recall but may lose context.\n",
    "4. Vector store indexes embeddings and metadata; examples: FAISS, Annoy, Milvus, Elasticsearch (dense vectors).\n",
    "5. (b) Principle of least privilege; sanitize input; constrain tools.\n",
    "6. Retrieve top-k then rerank by relevance model or exact match.\n",
    "7. System sets behavior, user asks, assistant answers.\n",
    "8. Summarize, retrieve selectively, compress, use shorter templates.\n",
    "9. (c)\n",
    "10. Faithfulness, answer groundedness, citation correctness, latency, cost.\n",
    "\n",
    "### Fill blanks\n",
    "1. -1, 1\n",
    "2. normalization\n",
    "3. chunking\n",
    "4. context\n",
    "5. dense\n",
    "6. candidate\n",
    "7. placeholders\n",
    "8. retrieval"
   ],
   "id": "493906b6b2873d0b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
