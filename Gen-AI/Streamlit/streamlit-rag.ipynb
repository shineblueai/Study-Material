{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlit RAG (Retrievalâ€‘Augmented Generation)\n",
    "Build an inâ€‘memory RAG app: upload files â†’ chunk â†’ embed â†’ index into FAISS â†’ ask questions â†’ get answers with citations. Supports OpenAI, Gemini, Anthropic, Groq, or local/HF embeddings.\n"
   ],
   "id": "6af5336131b3f303"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Installation (commented)",
   "id": "d47dc0a27f3e5b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# !pip install streamlit faiss-cpu pypdf chromadb tiktoken\n",
    "# !pip install sentence-transformers openai anthropic google-generativeai groq huggingface_hub\n"
   ],
   "id": "7b154ecf6503fea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "34e444d8d4f9092a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import streamlit as st\n",
    "from typing import List\n"
   ],
   "id": "23fcb24006ab2b21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optional heavy imports inside functions to avoid import costs on app load",
   "id": "4fd4fd3f92b603a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_pdf_text(file_bytes: bytes) -> str:\n",
    "    from pypdf import PdfReader\n",
    "    reader = PdfReader(io.BytesIO(file_bytes))\n",
    "    return \"\\n\".join([p.extract_text() or '' for p in reader.pages])\n",
    "\n",
    "def split_text(text: str, chunk_size=800, chunk_overlap=120) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return [c.strip() for c in chunks if c.strip()]\n",
    "\n",
    "def get_embedder(name: str):\n",
    "    if name == 'OpenAI':\n",
    "        from openai import OpenAI\n",
    "        if (k:=st.secrets.get('OPENAI_API_KEY', None) if hasattr(st,'secrets') else None) or os.environ.get('OPENAI_API_KEY'):\n",
    "            os.environ['OPENAI_API_KEY'] = k or os.environ.get('OPENAI_API_KEY','')\n",
    "        client = OpenAI()\n",
    "        def _emb(texts: List[str]):\n",
    "            resp = client.embeddings.create(model='text-embedding-3-small', input=texts)\n",
    "            return [d.embedding for d in resp.data]\n",
    "        return _emb\n",
    "    if name == 'SentenceTransformers':\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        return lambda texts: model.encode(texts, show_progress_bar=False, normalize_embeddings=True).tolist()\n",
    "    if name == 'HuggingFace Inference':\n",
    "        from huggingface_hub import InferenceClient\n",
    "        hf = InferenceClient(token=st.secrets.get('HUGGINGFACEHUB_API_TOKEN', os.environ.get('HUGGINGFACEHUB_API_TOKEN')))\n",
    "        return lambda texts: [hf.embeddings(model='sentence-transformers/all-MiniLM-L6-v2', inputs=[t]).data[0].embedding for t in texts]\n",
    "    raise ValueError('Unknown embedder')\n",
    "\n",
    "def build_faiss_index(vectors):\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    arr = np.array(vectors, dtype='float32')\n",
    "    dim = arr.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(arr)\n",
    "    index.add(arr)\n",
    "    return index\n",
    "\n",
    "def search_index(index, query_vec, k=5):\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    q = np.array([query_vec], dtype='float32')\n",
    "    faiss.normalize_L2(q)\n",
    "    scores, idxs = index.search(q, k)\n",
    "    return scores[0], idxs[0]\n",
    "\n",
    "def generate_answer(provider: str, model: str, question: str, contexts: List[str], temperature: float, max_tokens: int) -> str:\n",
    "    system = (\n",
    "        \"You are a helpful assistant. Answer using only the provided context snippets. \"\n",
    "        \"If the answer is not contained in the context, say you don't know.\"\n",
    "    )\n",
    "    context_block = \"\\n\\n\".join([f\"[Source {i+1}]\\n{c}\" for i,c in enumerate(contexts)])\n",
    "    prompt = f\"Context:\\n{context_block}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    if provider == 'OpenAI':\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        res = client.chat.completions.create(model=model, temperature=temperature, max_tokens=max_tokens,\n",
    "                                             messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":prompt}])\n",
    "        return res.choices[0].message.content\n",
    "    if provider == 'Gemini':\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=st.secrets.get('GEMINI_API_KEY', os.environ.get('GEMINI_API_KEY')))\n",
    "        g = genai.GenerativeModel(model)\n",
    "        out = g.generate_content(f\"{system}\\n\\n{prompt}\")\n",
    "        return getattr(out,'text',str(out))\n",
    "    if provider == 'Anthropic':\n",
    "        import anthropic\n",
    "        a = anthropic.Anthropic()\n",
    "        out = a.messages.create(model=model, max_tokens=max_tokens, temperature=temperature, system=system,\n",
    "                                messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        return out.content[0].text\n",
    "    if provider == 'Groq':\n",
    "        from groq import Groq\n",
    "        gq = Groq(api_key=st.secrets.get('GROQ_API_KEY', os.environ.get('GROQ_API_KEY')))\n",
    "        out = gq.chat.completions.create(model=model, temperature=temperature, max_tokens=max_tokens,\n",
    "                                         messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":prompt}])\n",
    "        return out.choices[0].message.content\n",
    "    if provider == 'HuggingFace':\n",
    "        from huggingface_hub import InferenceClient\n",
    "        hf = InferenceClient(token=st.secrets.get('HUGGINGFACEHUB_API_TOKEN', os.environ.get('HUGGINGFACEHUB_API_TOKEN')))\n",
    "        try:\n",
    "            resp = hf.chat_completion(model=model, messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":prompt}], max_tokens=max_tokens)\n",
    "            return resp.choices[0].message['content'] if hasattr(resp.choices[0],'message') else resp.choices[0]['message']['content']\n",
    "        except Exception:\n",
    "            return hf.text_generation(model=model, inputs=f\"{system}\\n\\n{prompt}\", max_new_tokens=max_tokens)\n",
    "    raise ValueError('Unsupported provider')\n"
   ],
   "id": "e7503fa29e54fd02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UI",
   "id": "81cb3d6782179308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st.set_page_config(page_title=\"RAG\", page_icon=\"ðŸ“š\")\n",
    "st.title(\"ðŸ“š RAG â€” Upload, Index, Ask\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"LLM & Embeddings\")\n",
    "    provider = st.selectbox('LLM Provider', ['OpenAI','Gemini','Anthropic','Groq','HuggingFace'], index=0)\n",
    "    if provider=='OpenAI':\n",
    "        model = st.text_input('LLM Model', 'gpt-4o-mini')\n",
    "    elif provider=='Gemini':\n",
    "        model = st.text_input('LLM Model', 'gemini-1.5-flash')\n",
    "    elif provider=='Anthropic':\n",
    "        model = st.text_input('LLM Model', 'claude-3-5-sonnet-20241022')\n",
    "    elif provider=='Groq':\n",
    "        model = st.text_input('LLM Model', 'llama-3.1-8b-instant')\n",
    "    else:\n",
    "        model = st.text_input('LLM Model', 'meta-llama/Llama-3.2-1B')\n",
    "    emb_backend = st.selectbox('Embedding backend', ['SentenceTransformers','OpenAI','HuggingFace Inference'], index=0)\n",
    "    chunk_size = st.slider('Chunk size', 200, 2000, 800, 50)\n",
    "    chunk_overlap = st.slider('Chunk overlap', 0, 400, 120, 10)\n",
    "    temperature = st.slider('temperature', 0.0, 1.5, 0.2, 0.1)\n",
    "    max_tokens = st.slider('max tokens', 64, 2048, 256, 32)\n",
    "\n",
    "uploaded = st.file_uploader(\"Upload files (PDF/TXT/MD)\", type=['pdf','txt','md'], accept_multiple_files=True)\n"
   ],
   "id": "9f767dfb94990243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build index on demand",
   "id": "a4630a2a2d9a6fed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if st.button('Build index'):\n",
    "    texts = []\n",
    "    for f in uploaded or []:\n",
    "        try:\n",
    "            if f.name.lower().endswith('.pdf'):\n",
    "                texts.append(load_pdf_text(f.read()))\n",
    "            else:\n",
    "                texts.append(f.read().decode('utf-8', errors='ignore'))\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Failed to read {f.name}: {e}\")\n",
    "    corpus = \"\\n\\n\".join(texts)\n",
    "    chunks = split_text(corpus, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    st.session_state['rag_chunks'] = chunks\n",
    "    embed = get_embedder(emb_backend)\n",
    "    vectors = embed(chunks)\n",
    "    index = build_faiss_index(vectors)\n",
    "    st.session_state['rag_index'] = index\n",
    "    st.session_state['rag_vectors'] = vectors\n",
    "    st.success(f\"Indexed {len(chunks)} chunks.\")\n"
   ],
   "id": "4e3688068989466f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ask questions",
   "id": "327f5573b50bfb44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "q = st.text_input(\"Ask a question about your documents\")\n",
    "if st.button('Answer') and q:\n",
    "    if 'rag_index' not in st.session_state:\n",
    "        st.warning('Please build the index first.')\n",
    "    else:\n",
    "        embed = get_embedder(emb_backend)\n",
    "        qvec = embed([q])[0]\n",
    "        scores, idxs = search_index(st.session_state['rag_index'], qvec, k=5)\n",
    "        chunks = st.session_state['rag_chunks']\n",
    "        ctx = [chunks[i] for i in idxs if 0 <= i < len(chunks)]\n",
    "        try:\n",
    "            ans = generate_answer(provider, model, q, ctx, temperature, max_tokens)\n",
    "        except Exception as e:\n",
    "            ans = f\"Error: {e}\"\n",
    "        st.subheader('Answer')\n",
    "        st.write(ans)\n",
    "        st.subheader('Citations')\n",
    "        for i, (c, s) in enumerate(zip(ctx, scores)):\n",
    "            with st.expander(f\"Source {i+1} (score {float(s):.3f})\"):\n",
    "                st.write(c)\n"
   ],
   "id": "dd25254cd9cff56b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "# - Everything is in-memory; for persistence use Chroma/Milvus etc.\n",
    "# - For larger docs, prefer background indexing and caching."
   ],
   "id": "cf69d77edb37c889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1aaedb499787a036"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
