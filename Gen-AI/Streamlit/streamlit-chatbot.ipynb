{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlit LLM Chatbot â€” Multiâ€‘provider\n",
    "This app provides a Streamlit UI for chatting with various LLM providers (OpenAI, Google Gemini, Anthropic, Groq, Hugging Face Inference API). Set your API keys as environment variables or Streamlit secrets.\n"
   ],
   "id": "d12b8f0eb74ff87f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation (commented)\n",
    "# - Run in terminal if needed\n",
    "# - pip install streamlit openai anthropic google-generativeai groq huggingface_hub"
   ],
   "id": "f33bfb0aaae8995f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install streamlit openai anthropic google-generativeai groq huggingface_hub tiktoken\n",
   "id": "c40e4dd4e60580eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# App: imports and env",
   "id": "858115a6b1e11cc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import streamlit as st\n"
   ],
   "id": "13e4a52379d2a7cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helpers: provider availability checks",
   "id": "462cd9d3603f3671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def has_key(name: str) -> bool:\n",
    "    # Prefer Streamlit secrets, then env var\n",
    "    try:\n",
    "        return bool(st.secrets.get(name) or os.environ.get(name))\n",
    "    except Exception:\n",
    "        return bool(os.environ.get(name))\n"
   ],
   "id": "dad09f923e275168"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helpers: client factories (lazy imports)",
   "id": "3a855b70987adc91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_openai_client():\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        api_key = st.secrets.get('OPENAI_API_KEY', None) if hasattr(st, 'secrets') else None\n",
    "        if not api_key:\n",
    "            api_key = os.environ.get('OPENAI_API_KEY')\n",
    "        if api_key:\n",
    "            os.environ['OPENAI_API_KEY'] = api_key\n",
    "        return OpenAI()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OpenAI client error: {e}\")\n",
    "\n",
    "def get_gemini_model(model_name='gemini-1.5-flash'):\n",
    "    import google.generativeai as genai\n",
    "    api_key = st.secrets.get('GEMINI_API_KEY', None) if hasattr(st, 'secrets') else None\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get('GEMINI_API_KEY')\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(model_name)\n",
    "\n",
    "def get_anthropic_client():\n",
    "    import anthropic\n",
    "    api_key = st.secrets.get('ANTHROPIC_API_KEY', None) if hasattr(st, 'secrets') else None\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "    if api_key:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = api_key\n",
    "    return anthropic.Anthropic()\n",
    "\n",
    "def get_groq_client():\n",
    "    from groq import Groq\n",
    "    api_key = st.secrets.get('GROQ_API_KEY', None) if hasattr(st, 'secrets') else None\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get('GROQ_API_KEY')\n",
    "    return Groq(api_key=api_key)\n",
    "\n",
    "def get_hf_client():\n",
    "    from huggingface_hub import InferenceClient\n",
    "    token = st.secrets.get('HUGGINGFACEHUB_API_TOKEN', None) if hasattr(st, 'secrets') else None\n",
    "    if not token:\n",
    "        token = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "    return InferenceClient(token=token)\n"
   ],
   "id": "d12fe3c29204511a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UI â€” Sidebar configuration",
   "id": "4578acb951335ed1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st.set_page_config(page_title=\"LLM Chatbot\", page_icon=\"ðŸ’¬\", layout=\"centered\")\n",
    "st.title(\"ðŸ’¬ LLM Chatbot â€” Multiâ€‘provider\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Provider & Model\")\n",
    "    provider = st.selectbox(\n",
    "        \"Choose provider\",\n",
    "        [\n",
    "            \"OpenAI\",\n",
    "            \"Gemini\",\n",
    "            \"Anthropic\",\n",
    "            \"Groq\",\n",
    "            \"HuggingFace\",\n",
    "        ],\n",
    "        index=0,\n",
    "    )\n",
    "    temperature = st.slider(\"temperature\", 0.0, 1.5, 0.7, 0.1)\n",
    "    max_tokens = st.slider(\"max tokens\", 64, 2048, 256, 32)\n",
    "    st.caption(\"Set API keys in st.secrets or environment variables.\")\n",
    "\n",
    "    if provider == \"OpenAI\":\n",
    "        model = st.text_input(\"Model\", value=\"gpt-4o-mini\")\n",
    "        key_ok = has_key('OPENAI_API_KEY')\n",
    "    elif provider == \"Gemini\":\n",
    "        model = st.text_input(\"Model\", value=\"gemini-1.5-flash\")\n",
    "        key_ok = has_key('GEMINI_API_KEY')\n",
    "    elif provider == \"Anthropic\":\n",
    "        model = st.text_input(\"Model\", value=\"claude-3-5-sonnet-20241022\")\n",
    "        key_ok = has_key('ANTHROPIC_API_KEY')\n",
    "    elif provider == \"Groq\":\n",
    "        model = st.text_input(\"Model\", value=\"llama-3.1-8b-instant\")\n",
    "        key_ok = has_key('GROQ_API_KEY')\n",
    "    else:\n",
    "        model = st.text_input(\"Model\", value=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "        key_ok = has_key('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "    if not key_ok:\n",
    "        st.warning(\"Missing API key for selected provider. Set it in secrets or env.\")\n",
    "\n",
    "system_prompt = st.text_area(\"System instruction (optional)\", value=\"You are a helpful assistant.\")\n"
   ],
   "id": "ae4082a9a55d527c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize chat history",
   "id": "a289664b9d1ab41e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []  # list of {role, content}\n"
   ],
   "id": "2401851023ce41ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chat input and display",
   "id": "b124d16b33bac17b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for m in st.session_state.messages:\n",
    "    with st.chat_message(m['role']):\n",
    "        st.markdown(m['content'])\n",
    "\n",
    "user_msg = st.chat_input(\"Type your messageâ€¦\")\n"
   ],
   "id": "b6f21fdb8260a509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dispatch call to chosen provider",
   "id": "39dc71f70fca3c7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def call_provider(provider: str, model: str, messages, temperature: float, max_tokens: int) -> str:\n",
    "    # messages is list of dicts [{'role': 'system'|'user'|'assistant', 'content': '...'}]\n",
    "    if provider == 'OpenAI':\n",
    "        client = get_openai_client()\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    if provider == 'Gemini':\n",
    "        model_obj = get_gemini_model(model)\n",
    "        # Gemini expects flattened text history; we'll join for simplicity\n",
    "        prompt = \"\\n\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        out = model_obj.generate_content(prompt)\n",
    "        return getattr(out, 'text', str(out))\n",
    "    if provider == 'Anthropic':\n",
    "        client = get_anthropic_client()\n",
    "        # Convert messages to Anthropic format (system separate)\n",
    "        sys = None\n",
    "        conv = []\n",
    "        for m in messages:\n",
    "            if m['role'] == 'system':\n",
    "                sys = m['content']\n",
    "            elif m['role'] in ('user', 'assistant'):\n",
    "                conv.append({'role': m['role'], 'content': m['content']})\n",
    "        resp = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            system=sys,\n",
    "            messages=conv,\n",
    "        )\n",
    "        return resp.content[0].text if getattr(resp, 'content', None) else str(resp)\n",
    "    if provider == 'Groq':\n",
    "        client = get_groq_client()\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    if provider == 'HuggingFace':\n",
    "        hf = get_hf_client()\n",
    "        # Use chat_completion if supported, else fall back to text_generation\n",
    "        try:\n",
    "            resp = hf.chat_completion(model=model, messages=messages, max_tokens=max_tokens, temperature=temperature)\n",
    "            return resp.choices[0].message['content'] if hasattr(resp.choices[0], 'message') else resp.choices[0]['message']['content']\n",
    "        except Exception:\n",
    "            # Flatten messages for text_generation\n",
    "            prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "            out = hf.text_generation(model=model, inputs=prompt, max_new_tokens=max_tokens, temperature=temperature)\n",
    "            return out\n",
    "    raise ValueError(f\"Unsupported provider: {provider}\")\n"
   ],
   "id": "6238e6f18332297f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handle user input",
   "id": "864a14cf87a1710a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if user_msg:\n",
    "    # Build message list for provider\n",
    "    msgs = []\n",
    "    if system_prompt:\n",
    "        msgs.append({'role': 'system', 'content': system_prompt})\n",
    "    msgs.extend(st.session_state.messages)\n",
    "    msgs.append({'role': 'user', 'content': user_msg})\n",
    "\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(user_msg)\n",
    "    with st.chat_message('assistant'):\n",
    "        placeholder = st.empty()\n",
    "        try:\n",
    "            reply = call_provider(provider, model, msgs, temperature, max_tokens)\n",
    "        except Exception as e:\n",
    "            reply = f\"Error: {e}\"\n",
    "        placeholder.markdown(reply)\n",
    "    st.session_state.messages.append({'role': 'user', 'content': user_msg})\n",
    "    st.session_state.messages.append({'role': 'assistant', 'content': reply})\n"
   ],
   "id": "3fa40579be4a554f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "# - To run: streamlit run streamlit-chatbot.ipynb (requires Streamlit >= 1.38 support for notebooks) or export to .py via jupytext/nbconvert.\n",
    "# - Set API keys: OPENAI_API_KEY, GEMINI_API_KEY, ANTHROPIC_API_KEY, GROQ_API_KEY, HUGGINGFACEHUB_API_TOKEN."
   ],
   "id": "ddfaa049920b6292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30728cf1e4f7126a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
