{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gen-AI Commands — One command per code cell with purpose\n",
    "This notebook-style script lists practical commands for interacting with popular LLM providers (OpenAI, Google Gemini, Anthropic, Groq, Hugging Face), as well as using Transformers for NLP/CV/Audio, and building RAG + Agents with LangChain. Each code cell contains one primary command with an inline comment describing its purpose. Networked API calls and heavy model downloads are commented for safety.\n"
   ],
   "id": "3c0ac338ab78efdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation (commented — run in a notebook terminal/kernel if needed)\n",
    "# - Keep providers modular: install only what you need"
   ],
   "id": "25d9c3d1f616edd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install openai anthropic google-generativeai groq huggingface_hub transformers accelerate sentence-transformers\n",
   "id": "3fa02d9a50069e7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install langchain langchain-openai langchain-anthropic langchain-community faiss-cpu chromadb pypdf tiktoken\n",
   "id": "21671a1d095c05f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install pillow matplotlib numpy pydub pytube datasets  # optional utilities and datasets\n",
   "id": "b77221d1f9394e40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Environment variables (set these in your shell or notebook secrets)\n",
    "# export/export-like commands are platform-specific; here we show Python patterns"
   ],
   "id": "6a56e1b279e1c1a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import os  # access environment variables for API keys\n",
   "id": "b125a0a3f7fd02c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ.get('OPENAI_API_KEY')  # fetch OpenAI API key from env (do not hardcode)\n",
   "id": "1f8d235deefc3daf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ.get('ANTHROPIC_API_KEY')  # fetch Anthropic API key from env\n",
   "id": "18c0894b0414849b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ.get('GEMINI_API_KEY')  # fetch Google Generative AI (Gemini) API key from env\n",
   "id": "e91163705c4a56cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ.get('GROQ_API_KEY')  # fetch Groq API key from env\n",
   "id": "fc9e3e56caf751e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ.get('HUGGINGFACEHUB_API_TOKEN')  # fetch Hugging Face token from env\n",
   "id": "9297306cefe76241"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OpenAI — Chat Completions and more",
   "id": "470a17de70d14638"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from openai import OpenAI  # OpenAI Python SDK v1 (uncomment when installed)\n",
   "id": "f7a91f7c0495de27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client = OpenAI()  # create OpenAI client using OPENAI_API_KEY from env\n",
   "id": "e25b79b7c3c03ad4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.chat.completions.create(model='gpt-4o-mini', messages=[{'role': 'user', 'content': 'Say hi'}])  # basic chat completion\n",
   "id": "771d3f2fce516b83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.chat.completions.create(model='gpt-4o-mini', messages=[{'role':'user','content':'JSON please'}], response_format={'type':'json_object'})  # JSON mode response\n",
   "id": "c3283e96e393b963"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.chat.completions.create(model='gpt-4o-mini', messages=[{'role':'user','content':'Tell a short story'}], temperature=0.7, max_tokens=128, top_p=1.0, stop=None)  # sampling params\n",
   "id": "701592f020b90c39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.chat.completions.create(model='gpt-4o-mini', messages=[{'role':'user','content':'Stream a response'}], stream=True)  # streaming response (iterate chunks in practice)\n",
   "id": "929116a9b9d2e85e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.embeddings.create(model='text-embedding-3-small', input=['hello world'])  # create vector embeddings\n",
   "id": "26dd038e0d74d10e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.images.generate(model='gpt-image-1', prompt='a cute robot reading a book')  # image generation (commented for safety)\n",
   "id": "fa42e461446c2a8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.audio.speech.create(model='gpt-4o-mini-tts', voice='alloy', input='Hello!')  # text-to-speech (commented)\n",
   "id": "cc0ef4d76100dc76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# client.audio.transcriptions.create(model='gpt-4o-transcribe', file=open('audio.wav','rb'))  # speech-to-text (commented)\n",
   "id": "6534d88fadcb0a11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# client.chat.completions.create(  # function/tool calling pattern (commented)\n",
    "#   model='gpt-4o-mini',\n",
    "#   messages=[{'role':'user','content':'What\\'s the weather in Paris?'}],\n",
    "#   tools=[{'type':'function','function':{'name':'get_weather','parameters':{'type':'object','properties':{'city':{'type':'string'}},'required':['city']}}}]\n",
    "# )  # ask model to decide tool use via function schema\n"
   ],
   "id": "fc3bcde79cc63804"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Google Gemini — Text and Multimodal",
   "id": "e2852922159c4be1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# import google.generativeai as genai  # Google Generative AI SDK\n",
   "id": "aab61de7372791c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))  # configure Gemini client via API key\n",
   "id": "6766c1752d099f99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# model = genai.GenerativeModel('gemini-1.5-flash')  # choose a Gemini model\n",
   "id": "e4833c49462e0efe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# model.generate_content('Say hello!')  # single-turn text generation\n",
   "id": "549106a34aebbc14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# chat = model.start_chat()  # start a chat session for multi-turn conversations\n",
   "id": "1d949d6f4ad9d151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# chat.send_message('Summarize: Large Language Models are...')  # send a message in the chat session\n",
   "id": "83fdb5f30db26e48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# model.generate_content([  # multimodal input (text + image) — commented\n",
    "#   'Describe this image',\n",
    "#   { 'mime_type': 'image/png', 'data': open('image.png','rb').read() }\n",
    "# ])  # Gemini supports images in prompts\n"
   ],
   "id": "e8be1631c9ace4c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# model.generate_content('Hi', generation_config={'temperature':0.7,'max_output_tokens':128}, safety_settings={'HATE':'block'})  # control generation & safety\n",
   "id": "3e503142bb710342"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Anthropic — Claude models (Messages API)",
   "id": "22a640648c3e83dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# import anthropic  # Anthropic SDK\n",
   "id": "a2e65a5dc81c080"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# aclient = anthropic.Anthropic()  # create Anthropic client using ANTHROPIC_API_KEY\n",
   "id": "4d58dd5f724806c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# aclient.messages.create(model='claude-3-5-sonnet-20241022', max_tokens=200, temperature=0.7, messages=[{'role':'user','content':'Hello Claude!'}])  # basic message\n",
   "id": "eb68d92e4c5d2d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# aclient.messages.create(  # tool use with schema (commented)\n",
    "#   model='claude-3-5-sonnet-20241022',\n",
    "#   max_tokens=256,\n",
    "#   tools=[{'name':'get_weather','description':'Get weather for a city','input_schema':{'type':'object','properties':{'city':{'type':'string'}},'required':['city']}}],\n",
    "#   messages=[{'role':'user','content':'Weather in Tokyo?'}]\n",
    "# )  # Claude will respond with tool use if needed\n"
   ],
   "id": "9cee01d25d5b6262"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Groq — Low-latency OpenAI-compatible Chat Completions",
   "id": "7d1ef54f24f298df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from groq import Groq  # Groq SDK\n",
   "id": "b961111a8740ba2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# gclient = Groq(api_key=os.environ.get('GROQ_API_KEY'))  # init Groq client\n",
   "id": "292f5aac6bf7f661"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# gclient.chat.completions.create(model='llama-3.1-8b-instant', messages=[{'role':'user','content':'Hi from Groq!'}])  # chat completion on Groq\n",
   "id": "7c3d6c043025df15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hugging Face Inference API — Text, Embeddings, Vision",
   "id": "4fbd1480926bda21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from huggingface_hub import InferenceClient  # lightweight hosted inference client\n",
   "id": "e9b26d9ac6191796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hf = InferenceClient(token=os.environ.get('HUGGINGFACEHUB_API_TOKEN'))  # init HF client with token\n",
   "id": "497fb3f0d6235cbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hf.text_generation(model='meta-llama/Llama-3.2-1B', inputs='Write a haiku about code.', max_new_tokens=64, temperature=0.7)  # text generation\n",
   "id": "968d6dbe8298a634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hf.chat_completion(model='Qwen/Qwen2.5-1.5B-Instruct', messages=[{'role':'user','content':'Hello!'}], max_tokens=128)  # chat with small instruct model\n",
   "id": "98bcd80ee81459dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hf.embeddings(model='sentence-transformers/all-MiniLM-L6-v2', inputs=['hello world'])  # embedding vectors\n",
   "id": "5c708f38032b774e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hf.image_to_text(model='nlpconnect/vit-gpt2-image-captioning', image=open('image.jpg','rb'))  # image captioning (commented)\n",
   "id": "ae1e0877179ce097"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# hf.image_classification(model='google/vit-base-patch16-224', image=open('image.jpg','rb'))  # image classification (commented)\n",
   "id": "c50be9b4618cffdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers — Pipelines for NLP, CV, Audio (local or cached; may download models)\n",
    "# Note: These examples may trigger downloads; keep them commented until ready."
   ],
   "id": "d7bb56e5b4c250d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from transformers import pipeline  # generic task pipelines\n",
   "id": "4742563a68867140"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('sentiment-analysis')(\"I love open-source!\")  # sentiment analysis\n",
   "id": "4b0c151d9f67fb79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('ner', aggregation_strategy='simple')('Hugging Face Inc. is based in NYC.')  # named entity recognition\n",
   "id": "560fb0aab3d3fa73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('summarization', model='sshleifer/distilbart-cnn-12-6')(\"Long text ...\")  # text summarization (small-ish model)\n",
   "id": "f41af19867333a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('translation', model='Helsinki-NLP/opus-mt-en-fr')('Hello world')  # English → French translation\n",
   "id": "44315dd37b81cb72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('text-generation', model='gpt2', do_sample=True, max_new_tokens=50)('Once upon a time')  # text generation\n",
   "id": "85342baa5f06ebe1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('zero-shot-classification')(\"This is a tech article\", candidate_labels=['tech','sports','politics'])  # zero-shot classification\n",
   "id": "2fb8d4c612622677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('image-classification')(images='image.jpg')  # image classification (commented)\n",
   "id": "a8cd91938c8aca21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('object-detection')(images='image.jpg')  # object detection (commented)\n",
   "id": "f69535f2cc8c455b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pipeline('automatic-speech-recognition', model='openai/whisper-tiny')('audio.wav')  # ASR (commented)\n",
   "id": "3b5141b9b96611e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multimodal prompts with OpenAI and Gemini (Vision)",
   "id": "3ee3a8d10483f2c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# client.chat.completions.create(  # OpenAI: text + image input (commented)\n",
    "#   model='gpt-4o-mini',\n",
    "#   messages=[{'role':'user','content':[{'type':'text','text':'Describe this image'}, {'type':'image_url','image_url':{'url':'https://example.com/cat.png'}}]}]\n",
    "# )  # vision-capable prompt\n"
   ],
   "id": "92431169e693dfbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# model.generate_content(['Explain this diagram', {'mime_type':'image/png','data':open('diagram.png','rb').read()}])  # Gemini multimodal (commented)\n",
   "id": "a7d28c03f97e0a3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parameters — Common controls across providers\n",
    "# - temperature: randomness (higher = more creative)\n",
    "# - top_p: nucleus sampling (fraction of probability mass)\n",
    "# - max_tokens / max_output_tokens: generation length limit\n",
    "# - stop: stop sequences\n",
    "# - presence_penalty / frequency_penalty (OpenAI): reduce repetition\n",
    "# - response_format (OpenAI): enforce JSON mode\n",
    "# - safety_settings (Gemini): content moderation config\n",
    "# - tools / function calling: tool-use schema to get structured actions\n",
    "# - seed / deterministic flags (when available)"
   ],
   "id": "1966028044786f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "params = {\n",
    "  'temperature': 0.7,\n",
    "  'top_p': 1.0,\n",
    "  'max_tokens': 256,\n",
    "  'stop': None\n",
    "}  # placeholder dict summarizing common generation parameters\n"
   ],
   "id": "4fcd522b67dd70a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain — Setup and LLM wrappers",
   "id": "d6cd63a2615eba4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain_openai import ChatOpenAI  # OpenAI chat wrapper for LangChain\n",
   "id": "2ac47b5a80a78736"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.2)  # create an LLM object for chains/agents\n",
   "id": "491c2ae63d7e8b11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain.prompts import ChatPromptTemplate  # composable prompt templates\n",
   "id": "68dd8e48b09ec5ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# template = ChatPromptTemplate.from_messages([\n",
    "#   ('system','You are a helpful assistant.'),\n",
    "#   ('human','{question}')\n",
    "# ])  # define a chat-style prompt template with variables\n"
   ],
   "id": "6b519b622b51658f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# chain = template | llm  # LCEL: pipe prompt into model to form a simple chain\n",
   "id": "b99c341e3de05f61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# chain.invoke({'question':'What is RAG?'})  # run the chain once with inputs\n",
   "id": "2d951efb946223f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangChain — RAG (Retrieval-Augmented Generation)\n",
    "# Steps: Split → Embed → Index → Retrieve → Generate"
   ],
   "id": "5d051f09f7dfd43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain.text_splitter import RecursiveCharacterTextSplitter  # split documents\n",
   "id": "6705dccba6b84971"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain_community.embeddings import HuggingFaceEmbeddings  # local embeddings (or use OpenAIEmbeddings)\n",
   "id": "6953e8a6acbd3931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain_community.vectorstores import FAISS  # in-memory vector store\n",
   "id": "697f9938409c8a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# docs = [\"RAG augments LLMs with retrieval.\", \"Vector stores index embeddings for search.\"]  # tiny corpus for demo\n",
   "id": "c0ca7eb7be619b62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)  # create text splitter\n",
   "id": "41ee46d6bece85e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# chunks = splitter.create_documents(docs)  # split raw docs into chunks\n",
   "id": "2083c7bc8d50809a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# embed = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  # embedding model (may download)\n",
   "id": "9a8792fc2a4c7dcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# vs = FAISS.from_documents(chunks, embed)  # build FAISS index from embedded chunks\n",
   "id": "13d4fa09a30b2479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# retriever = vs.as_retriever(search_kwargs={'k':2})  # turn vector store into a retriever\n",
   "id": "2eba421f8cc312fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain.chains import RetrievalQA  # high-level RAG chain\n",
   "id": "a8dfe7286a881560"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)  # create RAG chain with LLM + retriever\n",
   "id": "1b98870e91cea6cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# rag_chain.invoke({'query':'Explain RAG in one sentence.'})  # run RAG query (commented)\n",
   "id": "89028f0ff37189a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain — Tools and Agents (ReAct-style)",
   "id": "58ac5bb15e7edb86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain.agents import Tool  # tool interface\n",
   "id": "205256df4160d569"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# def multiply(a: int, b: int) -> int: return a * b  # example Python tool (toy)\n",
   "id": "62db8104426a1eb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# tools = [Tool(name='multiply', func=lambda q: str(multiply(*map(int,q.split()))), description='Multiply two ints given as \"a b\"')]  # define tool list\n",
   "id": "3f80e7a2aea2054b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# from langchain.agents import create_react_agent, AgentExecutor  # create ReAct agent\n",
   "id": "a51b37c23bb1b133"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# react_prompt = ChatPromptTemplate.from_messages([\n",
    "#   ('system','Use the tools to answer.'), ('human','{input}')\n",
    "# ])  # simple prompt for the agent\n"
   ],
   "id": "ab0a66f0d9e4766d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# agent = create_react_agent(llm=llm, tools=tools, prompt=react_prompt)  # build the agent\n",
   "id": "d2f5de96153234ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)  # wrap agent into an executor\n",
   "id": "cb4cafa9792ada62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# agent_executor.invoke({'input':'3 7'})  # run the agent with a tool-using query (commented)\n",
   "id": "c9403f99e5d0ff5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vector stores alternatives (commented stubs)\n",
    "# - Chroma: from langchain_community.vectorstores import Chroma\n",
    "# - Milvus/Qdrant/Pinecone/Weaviate: external services; require installs and API keys"
   ],
   "id": "28b4815251ed9065"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Example: Chroma\n",
    "# # from langchain_community.vectorstores import Chroma\n",
    "# # chroma = Chroma.from_documents(chunks, embed)\n"
   ],
   "id": "876e1d47a9aaf46b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Safety, rate limits, and retries\n",
    "# - Never hardcode API keys; use environment variables or secret managers.\n",
    "# - Respect provider ToS and safety guidelines; configure moderation where applicable.\n",
    "# - Handle rate limits with exponential backoff (e.g., `tenacity`).\n",
    "# - Cache results during experimentation to save costs."
   ],
   "id": "1b56042707f0944e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Example retry with tenacity (commented)\n",
    "# # from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "# # @retry(wait=wait_exponential(min=1, max=10), stop=stop_after_attempt(5))\n",
    "# # def safe_call():\n",
    "# #     return client.chat.completions.create(model='gpt-4o-mini', messages=[{'role':'user','content':'ping'}])\n"
   ],
   "id": "5da244b533be54a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optional providers (commented) — check docs and install SDKs as needed\n",
    "# - Cohere: cohere\n",
    "# - Mistral: mistralai\n",
    "# - OpenRouter: openrouter\n",
    "# - xAI (Grok): xai\n",
    "# - Together AI: together\n",
    "# - Perplexity: perplexityai"
   ],
   "id": "5af9942d2b89c1e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Example stubs:\n",
    "# # import cohere\n",
    "# # co = cohere.Client(os.environ.get('COHERE_API_KEY'))\n",
    "# # co.chat(message='Hello')\n"
   ],
   "id": "91826dd5316d16f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Minimal local smoke test (no network) — confirm Python env works",
   "id": "c00b915d1536d5dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sum([1,2,3])  # simple local operation to ensure kernel is responsive\n",
   "id": "5c187e1b820ef18a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
