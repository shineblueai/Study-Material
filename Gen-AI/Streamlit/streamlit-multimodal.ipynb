{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlit Multimodal ‚Äî Vision, Audio (Captioning, VQA, TTS/STT)\n",
    "Interact with multimodal models: image captioning (HF Inference), visual Q&A (Gemini/OpenAI), and optional TTS/STT (commented for safety).\n"
   ],
   "id": "dac2e8961cbe429f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Installation (commented)",
   "id": "5f1b2ecd4c1b1e4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install streamlit pillow huggingface_hub google-generativeai openai\n",
   "id": "411735ab4e30c8bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports & helpers",
   "id": "30809c859cc4c028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "def get_hf_client():\n",
    "    from huggingface_hub import InferenceClient\n",
    "    return InferenceClient(token=st.secrets.get('HUGGINGFACEHUB_API_TOKEN', os.environ.get('HUGGINGFACEHUB_API_TOKEN')))\n",
    "\n",
    "def get_openai_client():\n",
    "    from openai import OpenAI\n",
    "    if (k:=st.secrets.get('OPENAI_API_KEY', None) if hasattr(st,'secrets') else None) or os.environ.get('OPENAI_API_KEY'):\n",
    "        os.environ['OPENAI_API_KEY'] = k or os.environ.get('OPENAI_API_KEY','')\n",
    "    return OpenAI()\n",
    "\n",
    "def get_gemini_model(name='gemini-1.5-flash'):\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=st.secrets.get('GEMINI_API_KEY', os.environ.get('GEMINI_API_KEY')))\n",
    "    return genai.GenerativeModel(name)\n"
   ],
   "id": "f15bfda275d783dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UI",
   "id": "93c5f2c2402a6a7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st.set_page_config(page_title=\"Multimodal\", page_icon=\"üñºÔ∏è\")\n",
    "st.title(\"üñºÔ∏èüéôÔ∏è Multimodal ‚Äî Captioning, VQA, TTS/STT\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header('VQA Provider')\n",
    "    vqa_provider = st.selectbox('Provider', ['Gemini','OpenAI'], index=0)\n",
    "    if vqa_provider=='Gemini':\n",
    "        vqa_model = st.text_input('Gemini model', 'gemini-1.5-flash')\n",
    "    else:\n",
    "        vqa_model = st.text_input('OpenAI model', 'gpt-4o-mini')\n",
    "    st.caption('Set HUGGINGFACEHUB_API_TOKEN, GEMINI_API_KEY, OPENAI_API_KEY in secrets/env.')\n",
    "\n",
    "tab1, tab2, tab3 = st.tabs([\"Image Captioning\", \"Visual Q&A\", \"Audio (TTS/STT)\"])\n",
    "\n",
    "with tab1:\n",
    "    st.subheader('Image Captioning (HF Inference API)')\n",
    "    img_file = st.file_uploader('Upload image', type=['png','jpg','jpeg','webp'])\n",
    "    model_name = st.text_input('Captioning model', value='nlpconnect/vit-gpt2-image-captioning')\n",
    "    if st.button('Caption image') and img_file is not None:\n",
    "        try:\n",
    "            hf = get_hf_client()\n",
    "            cap = hf.image_to_text(model=model_name, image=img_file)\n",
    "            st.image(Image.open(img_file), caption='Uploaded image', use_column_width=True)\n",
    "            st.markdown(f\"**Caption:** {cap.generated_text if hasattr(cap,'generated_text') else cap}\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Captioning failed: {e}\")\n",
    "\n",
    "with tab2:\n",
    "    st.subheader('Visual Question Answering (VQA)')\n",
    "    vqa_img = st.file_uploader('Upload image for VQA', type=['png','jpg','jpeg','webp'], key='vqa')\n",
    "    question = st.text_input('Your question about the image')\n",
    "    if st.button('Answer question') and vqa_img is not None and question.strip():\n",
    "        try:\n",
    "            if vqa_provider=='Gemini':\n",
    "                g = get_gemini_model(vqa_model)\n",
    "                # Gemini expects bytes; provide as part of content\n",
    "                img_bytes = vqa_img.read()\n",
    "                out = g.generate_content([\n",
    "                    question,\n",
    "                    { 'mime_type': vqa_img.type or 'image/png', 'data': img_bytes }\n",
    "                ])\n",
    "                ans = getattr(out,'text',str(out))\n",
    "            else:\n",
    "                client = get_openai_client()\n",
    "                # OpenAI vision via image_url is standard; for local, upload or use data URL (not supported here). We'll use a placeholder: show message.\n",
    "                st.info('OpenAI vision typically uses image URLs or base64; uploading to a temporary URL is required. Using Gemini path is recommended for local files.')\n",
    "                ans = 'Please use Gemini tab for local file VQA, or host the image and pass the URL to OpenAI.'\n",
    "            st.image(Image.open(io.BytesIO(img_bytes if vqa_provider=='Gemini' else vqa_img.read())), use_column_width=True)\n",
    "            st.markdown(f\"**Answer:** {ans}\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"VQA failed: {e}\")\n",
    "\n",
    "with tab3:\n",
    "    st.subheader('Audio ‚Äî Text to Speech (TTS) and Speech to Text (STT)')\n",
    "    st.markdown('- TTS/STT can incur costs and require additional system libs. Examples are shown commented for safety.')\n",
    "    st.code(\n",
    "        \"\"\"\n",
    "# OpenAI TTS (commented)\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# audio = client.audio.speech.create(model='gpt-4o-mini-tts', voice='alloy', input='Hello world')\n",
    "# with open('tts.wav','wb') as f: f.write(audio.read())\n",
    "\n",
    "# OpenAI STT (commented)\n",
    "# with open('audio.wav','rb') as f:\n",
    "#     text = client.audio.transcriptions.create(model='gpt-4o-transcribe', file=f)\n",
    "#     print(text.text)\n",
    "        \"\"\",\n",
    "        language='python'\n",
    "    )\n"
   ],
   "id": "8c6eae618d404a66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "# - HF image captioning requires an access token and model availability.\n",
    "# - Gemini supports direct byte uploads in prompts; OpenAI generally expects URLs for images.\n",
    "# - For production, use proper file storage and pass URLs to models; handle safety/size limits."
   ],
   "id": "b4529501e8c81545"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ecdda8be95f093f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
