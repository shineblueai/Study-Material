{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlit Embeddings & Semantic Search\n",
    "Paste text or upload files, build an inâ€‘memory vector index using OpenAI, SentenceTransformers, or HF Inference embeddings. Search semantically and inspect scores.\n"
   ],
   "id": "dae6226d4c3c6f41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Installation (commented)",
   "id": "f9ff14d4c7cab0ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install streamlit faiss-cpu sentence-transformers openai huggingface_hub pypdf\n",
   "id": "966489ddd27ff851"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports & helpers",
   "id": "5c11ac662d1fb00a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import io\n",
    "import streamlit as st\n",
    "from typing import List\n",
    "\n",
    "def read_file(file) -> str:\n",
    "    name = file.name.lower()\n",
    "    if name.endswith('.pdf'):\n",
    "        from pypdf import PdfReader\n",
    "        reader = PdfReader(io.BytesIO(file.read()))\n",
    "        return \"\\n\".join([p.extract_text() or '' for p in reader.pages])\n",
    "    else:\n",
    "        return file.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "def get_embedder(backend: str):\n",
    "    if backend == 'OpenAI':\n",
    "        from openai import OpenAI\n",
    "        if (k:=st.secrets.get('OPENAI_API_KEY', None) if hasattr(st,'secrets') else None) or os.environ.get('OPENAI_API_KEY'):\n",
    "            os.environ['OPENAI_API_KEY'] = k or os.environ.get('OPENAI_API_KEY','')\n",
    "        client = OpenAI()\n",
    "        return lambda texts: [d.embedding for d in client.embeddings.create(model='text-embedding-3-small', input=texts).data]\n",
    "    if backend == 'SentenceTransformers':\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        return lambda texts: model.encode(texts, show_progress_bar=False, normalize_embeddings=True).tolist()\n",
    "    if backend == 'HuggingFace Inference':\n",
    "        from huggingface_hub import InferenceClient\n",
    "        hf = InferenceClient(token=st.secrets.get('HUGGINGFACEHUB_API_TOKEN', os.environ.get('HUGGINGFACEHUB_API_TOKEN')))\n",
    "        return lambda texts: [hf.embeddings(model='sentence-transformers/all-MiniLM-L6-v2', inputs=[t]).data[0].embedding for t in texts]\n",
    "    raise ValueError('Unknown backend')\n",
    "\n",
    "def build_index(vectors):\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    arr = np.array(vectors, dtype='float32')\n",
    "    faiss.normalize_L2(arr)\n",
    "    index = faiss.IndexFlatIP(arr.shape[1])\n",
    "    index.add(arr)\n",
    "    return index\n",
    "\n",
    "def search(index, query_vec, k=5):\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    q = np.array([query_vec], dtype='float32')\n",
    "    faiss.normalize_L2(q)\n",
    "    scores, idxs = index.search(q, k)\n",
    "    return scores[0], idxs[0]\n"
   ],
   "id": "e6c594f37dc7e056"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# UI",
   "id": "ef10b9f6070abeda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st.set_page_config(page_title=\"Embeddings Search\", page_icon=\"ðŸ”Ž\")\n",
    "st.title(\"ðŸ”Ž Embeddings & Semantic Search\")\n",
    "\n",
    "with st.sidebar:\n",
    "    backend = st.selectbox('Embedding backend', ['SentenceTransformers','OpenAI','HuggingFace Inference'], index=0)\n",
    "    top_k = st.slider('Top K', 1, 20, 5, 1)\n",
    "\n",
    "st.subheader('Corpus')\n",
    "text_input = st.text_area('Paste text (one paragraph per line)', height=150)\n",
    "files = st.file_uploader('Or upload files (PDF/TXT/MD)', type=['pdf','txt','md'], accept_multiple_files=True)\n"
   ],
   "id": "b0fd358078c8fb19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build index",
   "id": "c25523d0586b6c75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if st.button('Build index'):\n",
    "    docs: List[str] = []\n",
    "    if text_input.strip():\n",
    "        docs += [ln.strip() for ln in text_input.splitlines() if ln.strip()]\n",
    "    for f in files or []:\n",
    "        try:\n",
    "            docs.append(read_file(f))\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Failed to read {f.name}: {e}\")\n",
    "    if not docs:\n",
    "        st.warning('Provide some text or files first.')\n",
    "    else:\n",
    "        embed = get_embedder(backend)\n",
    "        vecs = embed(docs)\n",
    "        idx = build_index(vecs)\n",
    "        st.session_state['emb_docs'] = docs\n",
    "        st.session_state['emb_index'] = idx\n",
    "        st.session_state['emb_embed'] = embed\n",
    "        st.success(f'Indexed {len(docs)} documents.')\n"
   ],
   "id": "ed0cb23826caacaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Query",
   "id": "60952e6c97d22ada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "q = st.text_input('Query text')\n",
    "if st.button('Search') and q:\n",
    "    if 'emb_index' not in st.session_state:\n",
    "        st.warning('Build the index first.')\n",
    "    else:\n",
    "        embed = st.session_state['emb_embed']\n",
    "        qv = embed([q])[0]\n",
    "        scores, idxs = search(st.session_state['emb_index'], qv, k=top_k)\n",
    "        docs = st.session_state['emb_docs']\n",
    "        for rank, (i, s) in enumerate(zip(idxs, scores), start=1):\n",
    "            if 0 <= i < len(docs):\n",
    "                with st.expander(f\"#{rank}  score={float(s):.3f}\"):\n",
    "                    st.write(docs[i])\n"
   ],
   "id": "cdd909d1bcf1a7bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notes\n",
    "# - Export/import index can be added (faiss write_index/read_index), commented for safety.\n",
    "# - For large corpora, use persistent vector DBs (Chroma, Milvus, Pinecone)."
   ],
   "id": "b1d992166f6f7209"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca0ae9622459fc8d"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
